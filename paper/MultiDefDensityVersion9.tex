\documentclass[noinfoline]{imsart}
%\documentclass[10pt,letterpaper]{article}


\usepackage{bm}
\usepackage{geometry}
\usepackage{graphics,epsfig,rotate,lscape,graphicx,amsmath,amsthm,amssymb,float,amsfonts,amsbsy,hyperref,delarray,sectsty,amsfonts,amscd,pifont}
\usepackage{color,multirow}
\usepackage{algorithmic}
\usepackage{algorithm}



\geometry{letterpaper,left=1.2in,right=1.2in,top=1.2in,bottom=1.1in}
\bibliographystyle{plain}
\allowdisplaybreaks
%\def\references{\bibliography{C:/hanstex/macros/bib/11-1-11}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{Prop}{Proposition}
\newtheorem{aside}{Aside}
\newtheorem{claim}{Claim}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\newcommand{\ed}{\end{document}}
\newcommand{\no}{\noindent}
\newcommand{\et}{\textit{et al. }}
\newcommand{\btab}{\begin{tabular}}
\newcommand{\etab}{\end{tabular}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\np}{\newpage}
\newcommand{\la}{\label}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}
\newcommand{\bay}{\begin{array}}
\newcommand{\eay}{\end{array}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\boldsymbol}
\newcommand{\nn}{\nonumber}
\newcommand{\sm}{\vspace{.2cm}}
\newcommand{\bla}{\textcolor{black}}
\newcommand{\blu}{\textcolor{blue}}
\newcommand{\red}{\textcolor{red}}

\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}
\renewcommand{\labelenumi}{(\roman{enumi})}
\newcommand{\Comment}[1]{\textcolor{blue}{\textsc{#1}}}


\def\Ver{1}
\def\LongVer{1}
%%------------------------------ begin long version
%\if\Ver\LongVer{ 
%{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
%
%{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
%} \fi
%%------------------------------ end long version




\newcommand{\be}{\begin{eqnarray}}
\newcommand{\ee}{\end{eqnarray}}
\renewcommand{\baselinestretch}{1}%{1.7}

%====================================================================================
\begin{document}
%============================================================



\begin{frontmatter}

\title{A general spline representation for nonparametric and semiparametric density estimates using diffeomorphisms}
\runtitle{Diffeomorphisms for density estimation}

\begin{aug}
  \author{\fnms{Ethan}  \snm{Anderes}\thanksref{a}\ead[label=e1]{anderes@stat.ucdavis.edu}}
    \and
  \author{\fnms{Marc} \snm{Coram}\thanksref{b}\ead[label=e2]{mcoram@stanford.edu}}

  \runauthor{Anderes and Coram}

 % \affiliation{University of California, Davis and Stanford University}

  \address[a]{Department of Statistics, University of California, Davis CA 95616, USA. \printead{e1}}

  \address[b]{Department of Health Research and Policy (Biostatistics),  Stanford University, Palo Alto, CA 94305, USA. \printead{e2}}

\end{aug}

\begin{abstract}
A theorem of McCann \cite{mcc:95} shows that for any two absolutely continuous probability measures on $\Bbb R^d$ there exists a monotone transformation sending one probability measure to the other. 
A consequence of this theorem, relevant to statistics,  is that density estimation can be recast in terms of transformations. In particular, one can fix any absolutely continuous probability measure, call it $\Bbb P$, and then reparameterize the whole class of absolutely continuous probability measures as monotone transformations from $\Bbb P$.
In this paper we utilize this reparameterization of densities, as monotone transformations from some $\Bbb P$, to construct semiparametric and nonparametric density estimates. We focus our attention on classes of transformations, developed in the image processing and computational anatomy literature,  which are smooth,  invertible and which have attractive computational properties. 
 The techniques developed for this class of transformations allow us to show that a penalized maximum likelihood estimate (PMLE) of a smooth transformation from $\Bbb P$ exists and has a finite dimensional characterization, similar to those results found in the spline literature. 
 These results are derived utilizing  an Euler-Lagrange characterization of the PMLE which also establishes a surprising connection to a generalization of Stein's lemma for characterizing the normal distribution.
\end{abstract}

\begin{keyword}
\kwd{Euler-Lagrange}
\kwd{density estimation}
\kwd{penalized maximum likelihood}
\kwd{diffeomorphism}
\end{keyword}

\end{frontmatter}





%
%\thispagestyle{empty}
%
%\begin{center}
%{\Large {\bf A general spline representation for nonparametric and semiparametric density estimates using diffeomorphisms.}}
%
%\vspace{.5in}
%
%Ethan Anderes$^{1}$  and Marc Coram$^{2}$\\
%
%\vspace{.25in}
%
%${}^1$Department of Statistics, University of California, Davis, CA 95616,
%USA\\e-mail: {\tt anderes@stat.ucdavis.edu}
%
%\vspace{.25in}
%
%${}^2$Health Research and Policy Department, Stanford University, Palo Alto, CA 94304,
%USA
%
%
%\end{center}
%
%\begin{footnotetext}[1]
%{Supported in part by National Science Foundation grant DMS-1007480}
%\end{footnotetext}
%\begin{footnotetext}[2]
%{Supported in part by National Institute of Health grant 5UL1 RR02574404}
%\end{footnotetext}
%
%\newpage
%
%\begin{abstract}
%A theorem of McCann \cite{mcc:95} shows that for any two absolutely continuous probability measures on $\Bbb R^d$ there exists a monotone transformation sending one probability measure to the other. 
%A consequence of this theorem, relevant to statistics,  is that density estimation can be recast in terms of transformations. In particular, one can fix any absolutely continuous probability measure, call it $\Bbb P$, and then reparameterize the whole class of absolutely continuous probability measures as monotone transformations from $\Bbb P$.
%In this paper we utilize this reparameterization of densities, as monotone transformations from some $\Bbb P$, to construct semiparametric and nonparametric density estimates. We focus our attention on classes of transformations, developed in the image processing and computational anatomy literature,  which are smooth,  invertible and which have attractive computational properties. 
% The techniques developed for this class of transformations allow us to show that a penalized maximum likelihood estimate (PMLE) of a smooth transformation from $\Bbb P$ exists and has a finite dimensional characterization, similar to those results found in the spline literature. 
% These results are derived utilizing  an Euler-Lagrange characterization of the PMLE which also establishes a surprising connection to a generalization of Stein's lemma for characterizing the normal distribution.
%\end{abstract}
%
%\noindent{\it Key words and phrases}: Density estimation, Euler-Lagrange, penalized maximum likelihood,  diffeomorphism. 
%
%\vspace{.2in}
%
%\noindent {\bf MSC 2010 Subject Classification: 62G07}
%
%\thispagestyle{empty}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\newpage

\section{Introduction}

\setcounter{page}{1}

  Smooth invertible transformations, or deformations, are fast becoming important tools in modern data analysis. 
  They have been used with spectacular success in the field of computational anatomy where time varying vector field flows, which generate deformations, are used to statistically analyze medical fMRI images and quantify abnormal morphological structure 
  (see  \cite{alla:07, 
 Beg:2006ly,  
 conf/miccai/BegMTY03,
  Beg:2005qf,  
   cao:05, 
   dup:98, 
 Grenander:1998:CAE:309082.309089,
 Miller:1999bh, 
 Miller:2006zr, 
 Miller:2001ve, 
 Trouve:1998ys,  
 ty:dq,  
 vaillant:04, 
you:10,
Younes:2008}, and references therein).
  % (see  \cite{you:10} and references therein). 
  In cosmology, deformations  are used to model gravitational distortions of the cosmic microwave background from  dark matter density fluctuations and have resulted in a deeper understanding of cosmic structure \cite{Das:2011uq}. 
%Moreover, finding good transformations of the data is an important part of almost any data analysis (variance stabilizing transformations, for example).  
Transformations or deformations also have the power  
 to recast the generic problem of density estimation to that of deformation estimation.  In particular, one can fix any absolutely continuous probability measure, call it $\Bbb P$, and then reparameterize the whole class of absolutely continuous probability measures as monotone transformations from $\Bbb P$ (this follows by results in \cite{mcc:95}).  The advantage of this new viewpoint is the flexibility in choosing the target measure $\Bbb P$ which can encode  prior information on the shape of true sampling distribution. 
 For example, if it is known that the data is nearly Gaussian then choosing a  Gaussian $\Bbb P$  along with a strong penalty on transformations that are far from the identity allows one to construct penalized maximum likelihood estimates which effectively shrink the resulting nonparametric estimate in the direction of the Gaussian target $\Bbb P$. Moreover, when there is no knowledge about the true sampling measure, one can simply choose any absolutely continuous $\Bbb P$ and still construct a completely nonparametric density estimate. 

 
% 
%  density estimation can be obtained by fixing a target probability measure, then estimating a deformation which pushes forward the sampling distribution to the target measure. Such a density estimate  can generate nonparametric and semiparametric density estimates while  also incorporating  shape information for the unknown sampling distribution.
%
  Recent work in \cite{and:11}, \cite{AnderesNychka} and  \cite{ElMoselhy20127815} utilize  this idea of representing classes of  probability measures as deformations $\phi:\Bbb R^d\rightarrow \Bbb R^d$ of a known target probability measure $\Bbb P$ on $\Bbb R^d$. 
 The principle difficulty when working with such models is the construction of a rich class of deformations   which are nonparametric, smooth,  invertible and which are amenable to optimization. In \cite{and:11} and \cite{AnderesNychka} the authors the utilize  the class of quasi-conformal maps to generate penalized maximum likelihood estimates of $\phi$. However, these tools were only developed for $\Bbb R^2$ with no clear generalization for higher dimension. 
 In \cite{ElMoselhy20127815} the authors use polynomial approximations to $\phi$ which could potentially violate the invertabliltiy requirements on $\phi$.
In this paper,  we circumvent these challenges by adapting the powerful tools developed by Grenander, Miller,  Younes, Trouv\'e and co-authors in the image processing and computational anatomy literature   (see \cite{you:10} and the references therein) to generate estimates of  $\phi$ with all the required properties: nonparametric flexibility, smoothness, invertability and computational tractability. 
We establish the existence of a penalized maximum likelihood estimate of $\phi$  which has a finite dimensional characterization  similar to those results found in the spline literature (see \cite{wahba:90}). This finite dimensional characterization is a key component  of the numerical computation of these estimates which are nominally defined as a infinite  dimensional minimizer of a penalized likelihood. Moreover, our results are derived utilizing  an Euler-Lagrange characterization of the PMLE which also establishes a surprising connection to a generalization of Stein's lemma for characterizing the normal distribution.

We start the paper in Section \ref{rich} with an overview of using the  dynamics of time varying vector field flows to generate a rich class of diffeomorphisms. Then in sections \ref{pmle} and \ref{EL} we define our penalized maximum likelihood estimate (PMLE) of $\phi$ and prove not  only existence, but also establish a  finite dimensional characterization which is key for numerically computing the resulting density estimate. In Section \ref{steinSection} we notice a surprising connection with the Euler-Lagrange equation for the PMLE of $\phi$ and a generalization of Stein's lemma for characterizing the normal distribution (see \cite{stein:04}). In sections \ref{npe} and \ref{spe} we give examples of our new density estimate, first as a nonparametric density estimate and second as a semiparametric density estimate where a finite dimensional model is used for the target probability measure $\Bbb P$. We finish the paper with an appendix which contains some technical details used for the proofs of the existence of the PMLE and for the finite dimensional characterization. 


%%%%%%%%%%%%%
% section
%%%%%%%%%%%%%%%%
\section{A rich class of diffeomorphisms}
\label{rich}

 In this section we give a brief overview of using the  dynamics of time varying vector field flows to generate rich classes of diffeomorphisms. These time varying flows have been utilized   in the field of computational anatomy and image processing (see \cite{Younes:2008}, and references therein) and have been shown to be very powerful  for developing algorithms which optimize a diverse range of objective functions defined over classes of diffeomorphism.  It is these tools that we adapt for density estimation and statistics.



A map $\phi\colon \Omega \rightarrow\Bbb R^d$ is said to be a $C^k(\Omega,\Bbb R^d)$ diffeomorphism of the open set $\Omega \subset \Bbb R^d$ if  $\phi$ is one-to-one, maps onto $\Omega$, and  $\phi, \phi^{-1}\in C^{k}(\Omega,\Bbb R^d)$.  In what follows we generate classes of diffeomorphisms by time varying vector field flows. In particular, let  $\{ v_t\}_{t\in [0,1]}$  be a time varying vector field in $\Bbb R^d$, where $t$ denotes  `time' so that for each $t$, $v_t$ is a function mapping $\Omega$ into $\Bbb R^d$. Under mild smoothness conditions there exists a unique class of diffeomorphisms of $\Omega$, denoted $\{ \phi_t^v\}_{t\in [0,1]}$, which satisfy the following ordinary differential equation
\begin{align}
\label{ood}
 \partial_t \phi_t^v(x) &= v_t(\phi_t^v(x))
 \end{align}
 with boundary condition $\phi_0^v(x)=x$, for all $x\in \Omega$ (see Theorem \ref{ExistFlow} below). The interpretation of these flows is that $\phi_t(x)$ represents the position of a particle at time $t$, which originated from location $x$ at time $t=0$, and flowed according the the instantaneous velocity given by $v_t$. It will be convenient to consider the diffeomorphism that maps time $t$ to some other time $s$, this will be denoted $\phi_{ts}^{ v}(x)\equiv \phi_{s}^{v}( {\phi_{t}^{ v}}^{-1}(x)) $.

For the remainder of the paper we will assume that at each time $t$, $v_t$ will be a member of a  Hilbert space of vector fields mapping  $\Omega$ into $\Bbb R^d$ with inner product denoted by $\langle\cdot, \cdot \rangle_V$ and norm by $\|  \cdot \|_V$. 
Indeed, how one chooses the Hilbert space $V$ will determine the smoothness properties of the resulting class of deformations $\{\phi_t\}_{t\in[0,1]}$.  Once the Hilbert space $V$ is fixed we can define the following set of time varying vector fields.

\begin{definition}  
\label{defV01}
 Let  $V^{[0,1]}$ denote the space of measurable functions $v_t(x)\colon [0,1]\times \Omega\rightarrow \Bbb R^d$ such that $v_t\in V$ for all $t\in [0,1]$ and $\int_0^1 \| v_t\|^2_V dt <\infty $. 
\end{definition}
One clear advantage of this class  is that it can be endowed with a Hilbert space inner product if $V$ is a Hilbert space. Indeed,
 $V^{[0,1]}$ is a Hilbert space with inner product  defined by $\langle v,h \rangle_{V^{[0,1]}} \equiv\int_0^1 \langle  v_t, h_t\rangle_V dt$ (see Proposition \ref{Hspace} in the Appendix or Proposition 8.17 in \cite{you:10}).
For the remainder of the paper we typically use $v$ or $w$ to denote  elements of  $ V^{[0,1]}$ and  $v_t$ or $w_t$ to denote the corresponding elements of $V$ at any fixed time $t$.
An important theorem found in \cite{you:10} relates the smoothness of $V$ to the smoothness of the resulting diffeomorphism. 
Before we state the theorem, some definitions will be prudent. The Hilbert space $V$ is said to be continuously embedded in another normed space $H$ (denoted $V\hookrightarrow H$) if $V\subset H$ and there exists a constant $c$ such that
\[ \| v \|_H \leq c \| v\|_V \]
for all $v\in V$ where $\|\cdot \|_H$ denotes the norm in $H$. 
 Also we let $C_0^k(\Omega,\Bbb R^d)$ denote the subset of $C^k(\Omega,\Bbb R^d)$ functions whose partial derivatives of order $k$ or less all have continuous extensions to zero at the boundary $\partial \Omega$.
\begin{theorem}[\cite{you:10}, \cite{dup:98}]
\label{ExistFlow}
If $V\hookrightarrow C_0^k(\Omega,\Bbb R^d)$, then for any $v\in V^{[0,1]}$ there exists a unique class of $C^k(\Omega,\Bbb R^d)$ diffeomorphisms $\{\phi^v_t\}_{t\in[0,1]}$ which satisfy (\ref{ood}) and  $\phi_0^v(x) = x$ for all $x\in \Omega$. 
\end{theorem}

To derive our finite dimensional characterization we will make the additional assumption that $V$ is a reproducing kernel Hilbert space of vector fields. This will guarantee the existence of a reproducing kernel
 $K( x,y)\colon \Omega \times \Omega \rightarrow \Bbb R^{n\times n}$ which can be used to compute the evaluation functional. In particular, $K$ has the property that for any $x\in \Omega$ and $f\in V$ the following identity holds $\langle  K(x,\cdot) p , f\rangle _V=  p^T f(x)$ for all column vectors $p\in \Bbb R^d$.
 To simplify the following computations we will only work with kernels of the form $K(x,y) = R(x,y)I_{d\times d}$ where $R:\Omega\times\Omega\rightarrow \Bbb R$ is a positive definite function and $I_{d\times d}$ is the $d$-by-$d$ identity matrix. 
 
 Now the class of diffeomorphisms we consider in this paper  corresponds to the set of all time varying vector field flows evaluated at $t=1$: $\phi^v_1$, where $v$ ranges through $V^{[0,1]}$. The class $V^{[0,1]}$ will be completely specified by the reproducing kernel $R(x,y)I_{d\times d}$ which has the flexibility to control the smoothness of the resulting maps $\phi_1^v$ through Theorem \ref{ExistFlow}.
 %%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%
\section{Penalized maximum likelihood estimation}
\label{pmle}


Formally, we model our data   $X_1,\ldots, X_n$ as independent samples of an {\it unknown} diffeomorphism $\phi^v_1$ of a {\it known} probably distribution $\Bbb P$ on $\Bbb R^d$. In particular,
\begin{equation}
\label{model1} 
X_1,\ldots,X_n\overset{iid}\sim \Bbb P\circ \phi_1^v
\end{equation} 
where  $\phi^v_1$ is generated by a time varying vector field flow $\{\phi^v_t\}_{t\in[0,1]}$ which satisfies (\ref{ood}) and $v\in V^{[0,1]}$.
%where $\Bbb P$ is a known measure on $\Bbb R^d$ (or at least known up to some finite dimensional parameterization) and $\phi$ is an unknown smooth bijection of $\Bbb R^d$.
As remarked in the introduction, by simply choosing any absolutely continuous measure  $\Bbb P$ the
model (\ref{model1}) is still completely nonparametric.  The advantage is that when partial information exists on the sampling distribution of the data, it can potentially be encoded in the choice of $\Bbb P$.
The notation $\Bbb P\circ \phi^v_1$, in (\ref{model1}), is taken to mean that the probability of $X\in A$ is given by   $ \Bbb P(\phi^v_1(A))$ where $\phi_1^v(A)=\{ \phi(x)\colon x\in A \}$. An important observation is that the model (\ref{model1}) implies that $\phi_1^v(X)\sim \Bbb P$. Therefore, one can imagine estimating $\phi_1^v$ by attempting to ``deform'' the data $X_1,\ldots,X_n$ by a transformation which satisfies
\[ \phi_1^v(X_1),\ldots,\phi_1^v(X_n)\overset{iid}\sim \Bbb P. \]
 %In this way the model (\ref{model1}) is can be seen to be exceedingly flexible and, depending on how one models $\phi$ and $\Bbb P$,  can range from fully nonparametric to small perturbations of parametric models. 
%One of the main difficulties when working with such a model is the invertibility condition on the maps $\phi$. The nonlinearity of this condition in $\Bbb R^d$ when $d>1$ makes constructing rich classes and optimizing over such classes difficult. 
%One of the early attempts at circumventing such difficulties, found in \cite{and:11}, utilized  the class of quasi-conformal maps to generate penalized maximum likelihood estimates of $\phi$. However, these tools were only developed for $\Bbb R^2$ with no clear generalization for higher dimension. 
%In this paper we adapt the powerful tools developed by Grenander, Miller,  Younes, Trouv\'e and co-authors in the image processing and computational anatomy literature   (see \cite{you:10} and the references therein) and 
%apply them to the estimation of $\phi$ from data $X_1,\ldots, X_n$. Indeed, these techniques allow us to show that a penalized maximum likelihood estimate of $\phi$ exists and that the solution has a finite dimensional characterization, similar to those results found in the spline literature (see \cite{wahba:90}). 
%
%
In this section, we construct a penalized maximum likelihood estimate (PMLE) of $\hat v$ given the data (\ref{model1}), whereby obtaining an estimate $ \Bbb P\circ \phi^{\hat v}_1$ of the true sampling  distribution.

The target probability measure $\Bbb P$ is assumed to have a bounded density with respect to Lebesque measure on $\Bbb R^d$. 
Therefore, by writing the density of $\Bbb P$ as $\exp H$ for some  function $H\colon\mathbb R^d\rightarrow \mathbb R\cup \{-\infty\}$, the probability measure $\Bbb P\circ \phi^v_1$ has density given by
 \[ d\,\Bbb P\circ \phi_1^v(x)  =\det (D\phi_1^v(x))  \exp H \circ \phi_1^v(x) dx  \]
 where $\det (D\phi^v_1(x))$ is defined as the  determinant of the Jacobian of $\phi^v_1$ evaluated at $x\in \Omega$ (always positive by the orientation preserving nature of $\phi^v_1$). 
Since $\phi_1^v$ ranges over an infinite dimensional space of diffeomorphisms, the  likelihood for $v$ given the data  will typically be unbounded as $v$ ranges in $V^{[0,1]}$. The natural solution is to regularize the log likelihood using the corresponding  Hilbert space norm on $V^{[0,1]}$ with a  multiplicative tuning factor $\lambda/2$. 
The penalized log-likelihood (scaled by $1/n$) for the unknown vector field $v$ flow given data $X_1,\ldots, X_n\overset{iid}\sim \Bbb P\circ \phi^v_1$ is then given by
 \begin{equation}
 \label{energy1}
 E_\lambda(v)\equiv  \frac{1}{n} \sum_{k=1}^n \log\text{det}[D\phi(X_k) ]+H\circ\phi(X_k)  - \frac{\lambda}{2}  \int_0^1 \| v_t \|_V^2 dt.
 \end{equation}
 The estimated vector field $\hat v$ is chosen to be any element of $V^{[0,1]}$ which maximizes $E_\lambda$ over $V^{[0,1]}$.
 The following theorem establishes that such a $\hat v$ exists.
 %%%%%%%%%%%
 % begin claim
 %%%%%%%%%%%%
\begin{claim}
\label{claim1}
Let $V$ be a  Hilbert space which is  continuously  embedded in $C_0^2(\Omega, \Bbb R^d)$ where $\Omega$ is a bounded open subset of $\Bbb R^d$. Suppose $e^{H(\cdot)}$ is a bounded and continuous density  on $\Omega$. Then there exists a time varying vector field $\hat v \in V^{[0,1]}$ such that 
\begin{equation}
\label{exist}
E_\lambda(\hat v)=\sup_{v\in V^{[0,1]}}E_\lambda(v).
\end{equation}
\end{claim}
%%%%%%%%%%
% begin proof
%%%%%%%%%%%%
\begin{proof} 



We first establish $\sup_{v\in V^{[0,1]}}E_\lambda(v)<\infty$ by splitting the energy $E_\lambda$ into three parts
\begin{equation}
\label{decomp}
E_\lambda(v)=\underbrace{\frac{1}{n}\sum_{k=1}^n   \log\text{det} D\phi_1^v(X_k) }_{=:E_1(v)}+ \underbrace{\frac{1}{n}\sum_{k=1}^n H\circ\phi_1^v(X_k)}_{=:E_2(v)}  \underbrace{-\frac{\lambda}{2} \int_0^1 \| v_t \|_V^2 dt.}_{=:E_3(v)}  
\end{equation}
Notice that each term is well defined and finite whenever $v\in V^{[0,1]}$, since
the assumption $V\hookrightarrow C_0^2(\Omega, \Bbb R^d)$ is sufficient for  Theorem 8.7   in \cite{you:10} to apply to the class  $ V^{[0,1]}$. In particular, for any  $v\in V^{[0,1]}$  there exists a unique class of $C^1$ diffeomorphisms of $\Omega$, $\{\phi_t^v\}_{t\in [0,1]}$, which satisfies (\ref{ood}) (also see Theorem 2.5 in \cite{dup:98}). 
The term $E_2(v)$ is clearly bounded from above since  $\sup_{x\in\Omega}H(x)<\infty$ by assumption.
For the remaining two terms notice that  the determinant of the Jacobian is given by   $ \log\det D\phi_1^v(x)  = \int_0^1  \text{div}\,  v_t (\phi_t^v(x))dt$ (by equation (\ref{div}) in the Appendix).
Therefore
\begin{align}
E_1(v) + E_3(v) & = \frac{1}{n}\sum_{k=1}^n   \int_0^1 \left(  \text{div}\,  v_t (\phi_t^v(X_k))  - \frac{\lambda}{2}   \| v_t \|_V^2
\right)dt \nonumber\\
 & \leq \frac{1}{n}\sum_{k=1}^n   \int_0^1 \left( \sup_{x\in \Omega} |\text{div}\,  v_t (x)|  - \frac{\lambda}{2}   \| v_t \|_V^2
\right)dt \nonumber \\
 & \leq    \int_0^1 \left( c\| v_t \|_V  - \frac{\lambda}{2}   \| v_t \|_V^2
\right)dt, \,\,\text{by the assumption $V\hookrightarrow C_0^2(\Omega, \Bbb R^d)$} \nonumber \\
& \leq \frac{c^2}{2\lambda}<\infty. \nonumber
\end{align}

Now let $v^1,v^2,\ldots$ be any maximizing sequence that satisfies $\lim_{m\rightarrow \infty} E(v^m) = \sup_{v\in V^{[0,1]}}E_\lambda(v)$. Since $\sup_{v\in V^{[0,1]}}E_\lambda(v)<\infty $  we can construct the sequence $v^m$ so that there exists an $M<\infty$ such that $\| v^m \|_{V^{[0,1]}}\leq M$ for all $m$. Since $\Omega$ is bounded, closed finite balls in $V^{[0,1]}=L^2([0,1],V)$ are weakly compact (by \cite{dup:98}). Therefore we may  extract a subsequence from $v^m$ (relabeled by $m$) which weakly converges to  a $\hat v \in V^{[0,1]}$. In particular, $\langle v^m, w \rangle_{V^{[0,1]}}\rightarrow \langle \hat v, w \rangle_{V^{[0,1]}}$ for all $w\in V^{[0,1]}$. Furthermore we have lower semicontinuity of the norm
\begin{equation}
\label{liminf}
 \liminf_{m\rightarrow \infty} \| v^m \|^2_{V^{[0,1]}} \geq  \| \hat v \|^2_{V^{[0,1]}}.
\end{equation}
Now by Theorem 3.1 in \cite{dup:98} 
 %------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
\cite{dup:98}  applies since the assumption $[W_0^{3,2}(\Omega)]^3$ (where $\Omega\subset \Bbb R^3$ in their paper) is only used to establish that $[W_0^{3,2}(G)]^3\hookrightarrow C_0^1(G)$. Since we are assuming $V\hookrightarrow C_0^2(\Omega, \Bbb R^d)\hookrightarrow C_0^1(\Omega, \Bbb R^d)$ we are free to use the results in \cite{dup:98} . 
{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%------------------------------ end long version
 we have that  $\phi^{v^m}_t(x) \rightarrow \phi^{\hat v}_t(x)$ uniformly in $t\in [0,1]$ as $m\rightarrow \infty$. 
This allows us to show that $\log \det D\phi_1^{v^m}(x)\overset{m\rightarrow\infty}\longrightarrow \log \det D\phi_1^{\hat v}(x)$ for every $x\in \Omega$. To see why, one can use similar reasoning as in \cite{cao:05}. First write
\begin{align*}
|\log \det D\phi_1^{v^m}(x)- \log \det D\phi_1^{\hat v}(x)| &= \left| \int_{0}^1 \text{div}\, v_t^m(\phi_t^{v^m}(x)) -  \text{div}\, \hat v_t(\phi_t^{\hat v}(x)) dt \right| 
=I + I\!I 
%\\
%& \leq \underbrace{\left| \int_{0}^1 \text{div}\, v_t^m(\phi_t^{v^m}(x)) -  \text{div}\, v^m_t(\phi_t^{\hat v}(x)) dt \right|}_{\equiv I} \\
%& \qquad\qquad\qquad + \underbrace{\left| \int_{0}^1 \text{div}\,  v^m_t(\phi_t^{\hat v}(x)) -  \text{div}\, \hat v_t(\phi_t^{\hat v}(x)) dt \right|.}_{\equiv I\!I} 
\end{align*}
where the first term $I$ satisfies 
\begin{align*}
I &\equiv \left| \int_{0}^1 \text{div}\, v^m_t(\phi_t^{v^m}(x)) -  \text{div}\, v^m_t(\phi_t^{\hat v}(x)) dt \right| \\
&\leq \int_{0}^1 \|\text{div}\, v^m_t\|_{1,\infty} \bigl|\phi_t^{v^m}(x) -  \phi_t^{\hat v}(x)\bigr| dt  \\
&\leq   \int_{0}^1 c\| v^m_t\|_{V} \bigl|\phi_t^{v^m}(x) -  \phi_t^{\hat v}(x)\bigr| dt,\,\,\text{since $V\hookrightarrow C_0^2(\Omega,\Bbb R^d)$} \\
&\leq  c\|  v^m\|_{V^{[0,1]}} \Bigl[\int_{0}^1 \underbrace{\bigl|\phi_t^{v^m}(x) -  \phi_t^{\hat v}(x)\bigr|^2}_\text{ $= o(1)$ uniformly in $t$} dt\Bigr]^{1/2},\,\,\text{by H\"older.} \\
&\rightarrow 0,\,\text{ since $\| v^m\|_{V^{[0,1]}}\leq M$ for all $m$.}
\end{align*}
For the second term $I\!I$ notice that  the map sending $v\mapsto \int_0^1 \text{div}\, v_t(y_t) dt$ is a bounded linear functional on $V^{[0,1]}$ (using the fact that $V\hookrightarrow C_0^1(\Omega, \Bbb R^d)$) where $y_t \equiv \phi_t^{\hat v}(x)$. By the Riesz representation theorem there exists a $w^{\hat v}\in V^{[0,1]}$ such that $\int_0^1 \text{div}\, v_t(y_t) dt = \langle v,w^{\hat v} \rangle_\text{\tiny $V^{[0,1]}$}$. Therefore  %there exists a $w^{\hat v}\in  V^{[0,1]}$ such that
 \begin{align*} 
I\!I 
&\equiv \left| \int_{0}^1 \text{div}\, v_t^m(\phi_t^{\hat v}(x)) -  \text{div}\, \hat v_t(\phi_t^{\hat v}(x)) dt \right|\\
&=   \Bigl| \langle v^m -  \hat v, w^{\hat v}\rangle_{V^{[0,1]}}\Bigr|\rightarrow 0, \,\,\text{by weak convergence.}
\end{align*}
Combining the results for $I$ and $I\!I$ we can conclude that $\log \det D\phi_1^{v^m}(x)\overset{m\rightarrow\infty}\longrightarrow \log \det D\phi_1^{\hat v}(x)$ for every $x\in \Omega$.

To finish the proof notice that
\begin{align}
\sup_{v\in V^{[0,1]}} E_\lambda(v) &= \lim_{m\rightarrow \infty} E(v^m)= \limsup_{m\rightarrow \infty} E(v^m)\nonumber\\
&= \frac{1}{n}\sum_{k=1}^n   \log\text{det} D\phi_1^{\hat v}(X_k)+ \frac{1}{n}\sum_{k=1}^n H\circ\phi_1^{\hat v}(X_k) -\frac{\lambda}{2} \liminf_{m\rightarrow \infty} \int_0^1 \| v^m_t \|_V^2 dt \nonumber\\
%&\phantom{\frac{1}{n}\sum_{k=1}^n   \log\text{det} D\phi_1^{\hat v}(X_k)+ \frac{1}{n}\sum_{k=1}^n H\circ\phi_1^{\hat v}(X_k) \frac{1}{n}\sum_{k=1}^n H\circ\phi_1^{\hat v}(X_k) }\text{ since $H$ is continuous} \\
&\leq \frac{1}{n}\sum_{k=1}^n   \log\text{det} D\phi_1^{\hat v}(X_k)+ \frac{1}{n}\sum_{k=1}^n H\circ\phi_1^{\hat v}(X_k) -\frac{\lambda}{2}  \int_0^1 \| \hat v_t \|_V^2 dt,\,\,\text{ by (\ref{liminf})}\nonumber \\
&= E_\lambda(\hat v) \nonumber
\end{align}

  \end{proof}
%%%%%%%%%%
% end proof
%%%%%%%%%%%%


One of the important facts about any vector field flow $\hat v\in V^{[0,1]}$ which maximizes $E_\lambda$  is that the resulting estimated transformation  $\phi_1^{\hat v}$ is a geodesic (or minimum energy) flow with respect to the vector field norm $\int_0^1 \| \hat v_t\|^2_V dt$. To see this is first notice that the parameterization of time $t=1$ maps, $\phi_1^v$, by vector fields $v\in V^{[0,1]}$ is a many-to-one  parameterization. 
%In particular there are multiple paths or flows which agree at time $t=0$ and at $t=1$, but differ at all times between. 
In other words there exist multiple pairs of vector fields $v,w\in V^{[0,1]}$ such that $\phi_1^v =\phi^w_1$ but $v\neq w$. Notice, however, that the log-likelihood term in $E_\lambda$ only depends on   $\phi_1^{\hat v}$. This implies that any maximizer $\hat v$ of $E_\lambda$ must simultaneously minimize  the penalty $\int_0^1 \| \hat v_t\|^2_V dt$ over the class of all $w\in V^{[0,1]}$ which has the same terminal value, i.e.\! $\phi^{\hat v}_1=\phi^w_1$.
 Consequently, the PMLE estimate $\hat v$ must be a geodesic flow. An important consequence  is that  geodesic flows $\{ \hat v_t \}_{t\in [0,1]}$ are completely determined by the initial vector field $\hat v_0$.  This will become particularly important in the next section where the initial velocity field will be completely parameterized by $n$ coefficient vectors.

%
%Notice  we are characterizing the map $\phi_t^{ v}$,  at time $t=1$, by the of vector fields $v_t$ as  $t$ ranges throughout $[0,1]$.  This implies that   characterizing maps $\phi_1^{ v}$ by elements $v\in V^{[0,1]}$ is  many-to-one parameterization. In particular, there exist distinct elements $v,w\in V^{[0,1]}$ with identical terminal maps, i.e.\!   $\phi_1^v=\phi^w_1$. Since the log-likelihood term only depends on   $\phi_1^{\hat v}$,  the penalty $\int_0^1 \| \hat v_t\|^2_V dt$ must attain a minimum over the class of all vector fields which have the same terminal value  $\phi_1^{\hat v}$ . Consequently, the PMLE estimate $\hat v$ must be a geodesic flow. An important consequence  is that  geodesic flows $\{ \hat v_t \}_{t\in [0,1]}$ are completely determined by the initial vector field $\hat v_0$.  This will become particularly important in the next section where the initial velocity field will be completely parameterized by $n$ coefficient vectors.
%
%
%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%
\section{Spline representation from Euler-Lagrange}
\label{EL}

In this section we work under the additional assumption that $V$ is a reproducing kernel Hilbert space. This assumption allows one to derive the Euler-Lagrange equation for any maximizer $\hat v$ of which satisfies (\ref{exist}). This leads to a finite dimensional characterization of  $\hat v$  which  parallel those results found in the spline literature for function estimation. 


%%%%%%%%%
%begin claim
%%%%%%%%%%%
\begin{claim}
\label{claim2}
Let $V$ be a  reproducing kernel Hilbert space, with  kernel $R(x,y)I_{d\times d}$, continuously  embedded in $C_0^3(\Omega, \Bbb R^d)$  where $\Omega$ is bounded open subset of $\Bbb R^d$. Suppose $e^{H(\cdot)}$ is a $C^1(\bar \Omega,\Bbb R)$ density  on $\Omega$. Then any time varying vector field $\hat v \in V^{[0,1]}$ which satisfies (\ref{exist}) also satisfies the following
 Euler-Lagrange equation: 
 \begin{align}
 \label{ELeq}
 \hat v_t(x)&=  \frac{1}{\lambda n}\sum_{k=1}^n \beta^T_{k,t} R(x,X_{k,t})  +  \frac{1}{\lambda n}\sum_{k=1}^n   \nabla_{y}^T R(x,y)\Bigr|_{y= X_{k,t}}
\end{align}
where $X_{k,t}\equiv \phi_t^{\hat v} (X_k)$, 
$ \beta_{k,t}\equiv   \nabla H(X_{k,1}) D\phi^{\hat v}_{t1}(X_{k,t})  +\nabla \log\det D\phi^{\hat v}_{t1} (X_{k,t})$ and $\nabla_y^T = (\partial_{y_1},\ldots, \partial_{y_d})^T$  is the transpose of the gradient operator applied to the $y$ variable.
\end{claim}
%%%%%%%%%%
% begin proof
%%%%%%%%%%%%
\begin{proof} 
Let $E_1, E_2$ and $E_3$ decompose $E_\lambda$ as in (\ref{decomp}). Notice first that if  $h\in V^{[0,1]}$  and $\epsilon \in \Bbb R$ then $2 E_3(\hat v+\epsilon h) = {\lambda}\| \hat v \|^2_{V^{[0,1]}} + \epsilon 2 \lambda\langle v,h \rangle_{V^{[0,1]}}  +\epsilon^2{\lambda}\| h \|^2_{V^{[0,1]}} $. Therefore $E_3(\hat v+\epsilon h)$ is differentiable with respect to $\epsilon$ with derivative given by
\begin{equation}
 \label{dderE1}
{\partial_\epsilon}  E_3(\hat v+\epsilon h)\bigr|_{\epsilon=0}
=  \int_0^1 \langle h_t,\lambda \hat v_t \rangle_V dt.
\end{equation}
%Proposition \ref{proo} (in the Appendix) 
In addition, Theorem 8.10 of \cite{you:10}
 implies that  $\phi^{\hat v+\epsilon h}_1(x)$  is differentiable at $\epsilon = 0$. % with derivative given by  (\ref{111}) in the Appendix.
 %with derivative given by  
%\begin{align}
%{\partial_\epsilon}  \phi^{\hat v+\epsilon h}_{1}(x)\bigr|_{\epsilon =0}  &= \int_0^1  \bigl\{D\phi^{ \hat v}_{u1} h_u \bigr\}\circ{\phi^{ \hat v}_{u}(x)}\,   du. \label{zzz}
% \end{align}
Now, the assumption  $H\in C^1(\bar\Omega)$ combined with equation (\ref{111}), in the Appendix, gives % that  $E_2(\hat v+\epsilon h)$ is differentiable at $\epsilon = 0$ and
\begin{align} 
\partial_\epsilon E_2(\hat v+\epsilon h)\bigr|_{\epsilon=0} \nonumber
%&= -\frac{1}{n}\sum_{k=1}^n \partial_\epsilon H(\phi_1^{\hat v+\epsilon h}(X_k)) \bigr|_{\epsilon=0}  \nonumber\\
&= -\frac{1}{n}\sum_{k=1}^n  \nabla H(\phi_1^{\hat v }(X_k)) \cdot {\partial_\epsilon}  \phi^{\hat v+\epsilon h}_{1}(X_k)\bigr|_{\epsilon = 0} \nonumber \\
&= -\frac{1}{n}\sum_{k=1}^n  \nabla H(\phi_1^{\hat v }(X_k)) \cdot \int_0^1  \bigl\{D\phi^{ \hat v}_{u1} h_u \bigr\}\circ{\phi^{ \hat v}_{u}(X_k)}\,   du \nonumber \\
%&= -\frac{1}{n}\sum_{k=1}^n \int_0^1   \nabla H( X_{k,1}) \cdot  \bigl\{D\phi^{ \hat v}_{u1} (X_{k,u}) h_u (X_{k,u})\bigr\}\,   du \nonumber \\
&= -\frac{1}{n}\sum_{k=1}^n \int_0^1  \bigl\{  \nabla H( X_{k,1}) D\phi^{\hat v}_{u1}(X_{k,u}) \bigr\} \cdot  h_u (X_{k,u})\,   du \nonumber \\
%&= -\frac{1}{n}\sum_{k=1}^n \int_0^1\bigl\langle \nabla H(q_{k,1}),  \bigl\{D\phi^v_{t,1}h_t \bigr\}(q_{k,t}) \bigr\rangle_d \,dt, \quad\text{by Lemma \ref{variationofphi}}\\
%&= -\frac{1}{n}\sum_{k=1}^n \int_0^1\bigl\langle  \bigl[D\phi^v_{t,1}(q_{k,t})\bigr]^{T}\nabla H(q_{k,1}),  h_t(q_{k,t}) \bigr\rangle_d \,dt \\
&=  \int_0^1\Bigl\langle  h_u(\cdot),  -\frac{1}{n}\sum_{k=1}^n \bigl\{ \nabla H( X_{k,1}) D\phi^{\hat v}_{u1}(X_{k,u}) \bigr\}^T R(\cdot,X_{k,u})\Bigr\rangle_V \,du  \label{derE2}
\end{align}
Finally, Proposition \ref{proo}, from the Appendix, implies $E_3(\hat v+\epsilon h )$ is differentiable at $\epsilon = 0$ with derivative given by  
\begin{align}
\partial_\epsilon  E_1(\hat v+\epsilon h)\bigr|_{\epsilon=0} \nonumber
&  =- \frac{1}{n} \sum_{k=1}^n  \partial_\epsilon  \log \det D\phi_{1}^{\hat v+\epsilon h }(X_k)\bigr|_{\epsilon = 0} \nonumber\\
&=- \frac{1}{n} \sum_{k=1}^n   
 \int_0^1  \Bigl[ h_u \cdot \nabla \log\det D\phi_{u1}^{\hat v}  + \text{\rm div}\, h_u\Bigr] \circ  \phi^{\hat v}_{u}(X_k)  \, du \nonumber\\
% &=- \frac{1}{n} \sum_{k=1}^n    \int_0^1 \Bigl\langle h_u(\cdot), \nabla \log\det D\phi_{u1}^{\hat v}(X_{k,u})  R(\cdot,X_{k,u}) \Bigr\rangle_V+ \langle h_u(\cdot) ,\nabla_{y} R(\cdot,y)\bigr|_{y=X_{k,u}}\Bigr\rangle_V \,du  \\
&= \int_0^1 \Bigl\langle h_u(\cdot), - \frac{1}{n} \sum_{k=1}^n \left\{ \nabla \log\det D\phi_{u1}^{\hat v}(X_{k,u})\right\}^T  R(\cdot,X_{k,u})+\nabla_{y}^T R(\cdot,y)\bigr|_{y=X_{k,u}}\Bigr\rangle_V \,du \label{derE3} 
\end{align}
{\em Remark:} the above equation requires $\partial_{x_i } ( e_i\cdot h_u(x))  = \partial_{x_i}   \langle e_i R(\cdot, x),  h_u\rangle_V=  \langle e_i  \partial_{x_i} R(\cdot, x),  h_u\rangle_V$ which follows since $\text{div}\, h_u \in V$ by the assumption $V\hookrightarrow C_0^2(\Omega,\Bbb R^d)$ (see \cite{aro:50}).
Now from (\ref{dderE1}),  (\ref{derE2}) and (\ref{derE3}), the energy $ E_\lambda(\hat v+\epsilon h)$ is differentiable with respect to $\epsilon$ at $0$ and
\begin{equation}
\label{EEEl}
0={\partial_\epsilon}  E_\lambda(\hat v+\epsilon h)\bigr|_{\epsilon=0}= \langle \mathcal E ^{\hat v},h \rangle_{V^{[0,1]}}
\end{equation}
where
\begin{equation}
\label{CalE}
 \mathcal E ^{\hat v}_t =\lambda \hat v_t   - \frac{1}{n}\sum_{k=1}^n \beta^T_{k,t}  R(\cdot,X_{k,t})  - \frac{1}{n}\sum_{k=1}^n \nabla_{y}^T R(\cdot,y)\bigr|_{y=X_{k,t}}
 \end{equation}
with $\beta_{k,t}\equiv  \nabla H( X_{k,1}) D\phi^{\hat v}_{t1}(X_{k,t})  + \nabla \log\det D\phi_{t1}^{\hat v}(X_{k,t}) $.
Since $h\in V^{[0,1]}$ was arbitrary, equation (\ref{EEEl}) implies $\mathcal E^{\hat v}=0$, which then gives (\ref{ELeq}). {\em Remark:} we are using the fact that the zero function in a reproducing kernel space is point-wise zero since the evaluation functionals are bounded. 
\end{proof}
%%%%%%%%%%
% end proof
%%%%%%%%%%%%


   There are a few things things to note here. First, the Euler-Lagrange equation  (\ref{ELeq}) only implicitly characterizes $\hat v$ since it appears on both sides of the equality ($\beta_{k,t}$ and $X_{k,t}$ also  depend on $\hat v$). Regardless,  (\ref{ELeq}) is useful since it implies that $\hat v$ must lie within a known $n\times d$ dimensional sub-space of $V^{[0,1]}$. In particular, as discussed at the end of Section \ref{pmle}, the estimate $\{ \hat v_t\}_{t\in [0,1]}$ is completely characterized by it's value at time $t=0$, i.e.\! $\hat v_0$ (by the geodesic nature of $\hat v$). Restricting equation (\ref{ELeq}) to  $t=0$ one obtains
   \begin{align}
 \label{ELeq0}
 \hat v_0(x)&=  \frac{1}{\lambda n}\sum_{k=1}^n \beta^T_{k,0} R(x,X_{k})  +  \frac{1}{\lambda n}\sum_{k=1}^n   \nabla_{y}^T R(x,y)\Bigr|_{y= X_{k}}.
\end{align}
  Simply stated, $\hat v_0$ has a finite dimensional spline characterization with spline knots set at the observations $X_1,\ldots, X_n$. Therefore to recover $\{ \hat v_t \}_{t\in[0,1]}$ one simply needs to find the $n$ row vectors $\beta_{1,0},\dots, \beta_{n,0}$ which satisfy the following fixed point equation
        \begin{equation}
 \label{InitialMom}
  \beta_{k,0}= \nabla H(\phi^{\hat v}_1(X_k))  D\phi^{\hat v}_{1}(X_k)+ \nabla \log\det D\phi^{\hat v}_{1} (X_k)
  \end{equation}
 for all $k=1,\ldots,n$. 



%%%%%%
% section
%%%%%%%%%
\section{Connection to Stein's Method}
\label{steinSection}

The Euler-Lagrange equation given in (\ref{ELeq}) has a surprising connection with a generalization of Stein's lemma for characterizing the normal distribution (see \cite{stein:04}). The main connection is that the Euler Lagrange equation for the PMLE estimate $\hat v_t$, simplified at initial time $t=0$ and terminal time $t=1$, can be reinterpreted as an empirical version of a generalization of Stein's lemma. This is interesting in it's own right, however, the connection may also bear theoretical fruit for deriving asymptotic estimation bounds on the nonparametric and semiparametric estimates derived from $\hat v$. In this section we make this connection explicit  with the goal of of motivating and explaining the Euler-Lagrange equation for $\hat v$ derived above.

To relate $\hat v_t$ at $t=0$ with Stein's lemma, and more generally Stein's method for distributional approximation,  first notice that (\ref{InitialMom}) implies the coefficients $\beta_{k,0}$,  from the implicit equation  (\ref{ELeq0}) for  $\hat v$, satisfy
$\beta_{k,0}% &= \bigl[D\phi^{\hat v}_{1}(X_k)\bigr]^T \nabla H(\phi^{\hat v}_1(X_k))^T+ \nabla \log\det D\phi^{\hat v}_{1} (X_k) \\
 = \nabla \log \hat f(X_k)
$ where  $\hat f= e^{ H\circ \phi^{\hat v}_1}  |D\phi_1^{\hat v}| $ is the estimated density of $X$ using the pullback of the target measure with the estimated diffeomorphisms $\phi^{\hat v}_1$.
 Now by computing the inner product of  both sides of the Euler-Lagrange equation (\ref{ELeq0}) with any vector field $u\in V$ and applying the reproducing property of the kernel $R(\cdot,\cdot)$ one derives
 \begin{align}
\lambda \langle \hat v_0, u\rangle_V% &= \frac{1}{n}\sum_{k=1}^n\bigl\{ \beta_{k,0} u(X_k) + \text{div}\, u(X_k)\bigr\}\nonumber \\
&=\Bbb E_n\bigl\{ \nabla \log \hat f(X)\cdot  u(X) + \text{div}\, u(X)\bigr\}. \label{stein2} 
%\\&\approx E_{f}\bigl\{ \nabla \log \hat f(X)\cdot u(X) + \text{div}\, u(X)\bigr\}  \label{stein2}
\end{align}
where $\Bbb E_n$ denotes  expectation with respect to the empirical measure generated by the data: $\frac{1}{n}\sum_{k=1}^n \delta_{X_k}$. 
To relate with Stein first let $\Bbb E$ denote expectation with respect to the population density $f =    e^{ H\circ \phi} |D\phi| $ given in our basic model (\ref{model1}). Notice that a generalization of Stein's lemma shows that if the densities $f$ and $\hat f$ give rise to the same probability measure then
\begin{equation}
\label{ste}
0= \Bbb E\bigl\{ \nabla \log \hat f(X) \cdot u(X) + \text{div}\, u(X)\bigr\} 
 \end{equation}
for all $u$ in a large class of test functions $\mathcal U$ (see Proposition 4 in \cite{stein:04}).
For example, a simple consequence of Lemma 2 in \cite{stein:81} implies that when $\hat f$ is the density of a  $d$ dimensional Gaussian  distribution $\mathcal N_d(\hat\mu,1)$  and $X\sim \mathcal N_d(\mu,1)$ then $\hat\mu = \mu$ implies $ \Bbb E\bigl\{ -  (X-\hat\mu) \cdot u(X) + \text{div}\, u(X) \bigr\} =0$ 
for any bounded function $u:\Bbb R^d \rightarrow \Bbb R^d$ with bounded gradient. Stein's method, on the other hand, generally refers to  a technique for  bounding the distance between two probability measures $f$ and $\hat f$ using bounds on departures from a characterizing equation, such as  (\ref{ste}) for example (see \cite{chen:05} for an exposition). The bounds typically take the form
\begin{align} \sup_{h\in \mathcal H} \left| \int ( h f - h \hat f ) \right| &\leq \sup_{u\in \mathcal U} \bigl| \Bbb E\bigl\{ \nabla \log \hat f(X)\cdot u(X) + \text{div}\, u(X)\bigr\} \bigr| \label{ssmethod} 
 \end{align}
where $\mathcal H$ and $\mathcal U$ are two class of functions related through a set of differential equations.
 %Setting up this equation shows how deviations from (\ref{ste}) controls the distance distance of $f$ from $\hat f$ (measured by the left hand side of (\ref{ssmethod})).
  In our case, applying a H\"older's inequality to the Euler-Lagrange equation (\ref{stein2}) gives a bound on  right hand side of (\ref{ssmethod}) in terms of a regularization measurement on the PMLE $\hat v$ and an empirical process error:
    \begin{align}
    \label{reg1}
     \sup_{u\in \mathcal U} \bigl| \Bbb E\bigl\{ \nabla \log \hat f(X) \cdot u(X) + \text{div}\, u(X)\bigr\} \bigr|   &\leq \underbrace{ \lambda \| \hat v_0\|_V   \sup_{u\in \mathcal U} \|  u \|_V}_\text{regularization at $t=0$} + \underbrace{\sup_{u\in \mathcal U} \bigl| (\Bbb E -\Bbb E_n) \nu_{\hat f,u}\bigr|}_\text{ empirical process error}
 \end{align}
 where $\nu_{\hat f,u} = \nabla \log \hat f(X) u(X) + \text{div}\, u(X)$.  
 This makes it clear that  theoretical control of the PMLE estimate $\hat v_t$ at time $t=0$, using the Euler-Lagrange equation characterization (\ref{ELeq0}), allows asymptotic control of the distance between the estimated density $\hat f$ and the true density $f$.
 
 At terminal time $t=1$, there is a similar connection with Stein's lemma. In contrast to time $t=0$, which quantifies the distance between the estimated  and population densities $\hat f$ and $f$,  time $t=1$ quantifies the distance between $\phi(X)$ (the target measure) with $\phi_1^{\hat v}(X)$ (the push forward of the true population distribution though the estimated map).  To make the connection, one follows  the same line of argument as above to  find that  for any $u\in V$
\begin{align}
\lambda \langle \hat v_1, u\rangle_V &=  {\Bbb E}^{\hat v}_n\bigl\{ \nabla H(X)\cdot u(X ) + \text{div}\, u(X)\bigr\} \label{stein}
\end{align}
where ${\Bbb E}^{\hat v}_n$ denotes  expectation with respect to the empirical measure $\frac{1}{n}\sum_{i=1}^n \delta_{\phi^{\hat v}_1(X_k)}$, which is simply the push forward of the empirical measure $\frac{1}{n}\sum_{i=1}^n \delta_{X_k}$ through the estimated map $\phi^{\hat v}_1$.
Now the analog to (\ref{reg1})  becomes
   \begin{align}
   \label{reg2}
    \sup_{u\in \mathcal U} \bigl| {\Bbb E^{\hat v}}\bigl\{ \nabla H(X)\cdot  u(X) + \text{div}\, u(X)\bigr\} \bigr|   &\leq \underbrace{ \lambda \| \hat v_1\|_V  \sup_{u\in \mathcal U} \|  u \|_V }_\text{regularization at $t=1$} +\, {\sup_{u\in \mathcal U} \bigl| (\Bbb E^{\hat v} -\Bbb E_n^{\hat v}) \gamma_{u}\bigr|}
 \end{align}
 where $\gamma_{u} = \nabla H(X) u(X) + \text{div}\, u(X)$ and $\Bbb E^{\hat v}$ denotes expectation with respect to the push forward of the population density $f =    e^{ H\circ \phi} |D\phi| $ though the estimated map $\phi_1^{\hat v}$.  
Since the target measure $\Bbb P$ is assumed to have density $e^H$, this bounds the distributional distance between $\phi^{\hat v}_1(X)$ and $\Bbb P$ when $X\sim \Bbb P\circ \phi$. 


%The hithro discussions is to view the Euler-Lagrange equation in more heuristic meantful way. However, it also no only a potentially new method for deriving asymptotics but ...
%also a possible new view of maximum likelihood esitmation, the score esentiialy giving a Stein's equation.

%%%%%%%%%%%%%%%%%
% section
%%%%%%%%%   
\section{Nonparametric example}
\label{npe}
%Talk about how the complete form of the maximum is difficult to numerically construct with current algorithms since the geodesics need to fix the endpoint and also the sheer around the endpoints. Therefore we simplify the form of the maximum, effectvely creating a basis set (similar to B-splines). However, a novel featuer of our fixed point equation is that we can tell when we have enough knots. 

In this section we utilize the finite dimensional characterization of the PMLE $\hat v$ at time $t=0$, given in (\ref{ELeq0}), to construct nonparametric density estimates of the form $\hat f= e^{ H\circ \phi^{\hat v}_1}  |D\phi_1^{\hat v}| $ from {\em iid} samples $X_1,\ldots, X_n$.
%  {\em iid} samples $X_1,\ldots, X_n$ from some population density $f$. 
%under the model $f =    e^{ H\circ \phi} |D\phi| $ given in  (\ref{model1}). 
As was discussed in the introduction, so long as the target measure $\Bbb P$ is absolutely continuous, the assumption that   $X_1,\ldots, X_n \overset{iid}\sim \Bbb  P\circ \phi$ encompasses all absolutely continuous measures. Since the class of diffeomorphisms $\{\phi_1^v\colon v\in V^{[0,1]}\}$ is nonparametric, the estimate $\hat f= e^{ H\circ \phi^{\hat v}_1}  |D\phi_1^{\hat v}| $ is inherently nonparametric regardless of the choice of target probability measure $\Bbb P$ (with density $e^H$). In effect, the choice of target $\Bbb P$ specifies a shrinkage direction for the nonparametric estimate: larger values of $\lambda$ shrink $\hat f$ further toward the target $\Bbb P$.
In this section we illustrate the nonparametric nature of the density estimate $\hat f$, whereas the next section explores semiparametric estimation with parametric models on the target $\Bbb P$.     One  key feature of our methodology is the use of the Euler-Lagrange equation (\ref{ELeq}) as a stopping criterion for a gradient based optimization algorithm for constructing $\hat v$. In fact, to avoid computational challenges associated with generating geodesics  with initial velocities given by (\ref{ELeq0}), we consider a finite dimensional subclass of $V^{[0,1]}$ which have geodesics that are amenable to computation (and for which gradients are easy to compute). The key is that we use the Euler-Lagrange identity (\ref{ELeq}) to measure the richness of the subclass, within the larger infinite dimensional Hilbert space $V^{[0,1]}$, whereby allowing a dynamic choice  of the approximating dimension for a target resolution level.  


Claim~\ref{claim2} shows that the PMLE vector field $\hat{v}\in V^{[0,1]}$ obeys a parametric form determined up to the identification of the $n$ functions $t\mapsto \beta_{k,t}$ as $t$ ranges in $[0,1]$. Moreover, the whole path of coefficients $\beta_{k,t}$ is determined from the initial values $\beta_{k,0}$, by the geodesic nature of $\hat v$. 
In this way, we are free to optimize, over the vectors $\{\beta_{1,0}, \ldots, \beta_{n,0}\}\subset \Bbb R^d$ using equation (\ref{ELeq0}) 
%when the velocity fields of the form $\frac{1}{\lambda n}\sum_{k=1}^n \beta_{k,0}^T R(x,X_{k})  +  \frac{1}{\lambda n}\sum_{k=1}^n   \nabla_{y} R(x,y)\bigr|_{y= X_{k}}$
 and are guaranteed that the global maximum, over the full infinite dimensional space $\{ \phi_1^v\colon v\in V^{[0,1]}\}$, has this form.  Unfortunately, deriving geodesic maps with this type of initial velocity field is challenging.  
%In what follows we consider a subclass of initial vector fields which allow easy numerics, then use the Euler-Lagrane equation to determine if the subclass if sufficiently rich within the larger infinite dimensional space.
To circumvent this difficulty we choose an approximating  subclass of vector fields at time $t=0$ which are parametrized by the selection of $N$ knots $\{\kappa_{1},\ldots, \kappa_N\} \subset \Omega$ and $N$ initial momentum row vectors $\{\eta_{1},\ldots, \eta_N\} \subset  \mathbb{R}^d$ and have the form: 
\begin{equation}
\label{knots}
 v_0(x)= \sum_{k=1}^N \eta^T_{k} R(x,\kappa_{k}).
 \end{equation}
The knots $\{ \kappa_1,\ldots, \kappa_N\}$ need not be located at the data points $\{X_1,\ldots, X_n \}$. Indeed, we will see that alternative configurations of knots can be numerically beneficial. The key point is that vector fields at time $t=0$, which satisfy (\ref{knots}), generate geodesics with respect to norm $\bigl[\int_0^1 \| v^t  \|^2_V dt \bigr]^{1/2}$ that are easy to compute. Moreover, the variational derivatives of the terminal map $\phi^v_1$ with respect to the initial $\eta$ coefficients and the knots $\kappa$ are easily computed when utilizing  similar techniques  as those developed in \cite{vaillant:04} and \cite{alla:07}. This enables efficient gradient based algorithms for optimizing the PMLE criterion over the class generated by (\ref{knots}).


\begin{figure}[t]
\centering
\includegraphics[height = 2.2in]{pics/fig01/density_justn}
\includegraphics[height = 2.2in]{pics/fig01/agreement_justn}\\
\includegraphics[height = 2.2in]{pics/fig01/density_n_pm_h}
\includegraphics[height = 2.2in]{pics/fig01/agreement_n_pm_h}\\
\caption{In this example we compare two different knot configurations, in (\ref{knots}), for generating nonparametric density estimates using approximate solutions to the  Euler-Lagrange equation (\ref{ELeq}). The left column of images shows two different density estimates (red), based on the same data set (blue), using two different knot configurations (top-left uses $10$ knots, bottom-left uses $30$ knots). The right column of images show the corresponding  diagnostic curves which characterize the richness of the approximating subclass generated by the knots. The fact that the two diagnostic curves shown bottom-right are similar suggests that the $30$ knots used generate the approximating subclass by (\ref{knots}) is sufficiently rich to reach the stationary points of the penalized log likelihood $E_\lambda$ given in (\ref{energy1}).
See Section \ref{npe} for details.
 \label{f2}   
 }
\end{figure}



As a first illustration, we show that the na\"ive choice of initial knots obtained by setting $\{\kappa_1,\ldots, \kappa_N\}=\{ X_1,\ldots,X_n\}$ in (\ref{knots})  is {\em not} sufficient to solve  (\ref{ELeq}); then show how it can be easily fixed using the Euler-Lagrange methodology. 
 Our data set, shown with blue sticks in Figure~\ref{f2}, consists of $n=10$ independent samples from a mixture of two normals, truncated so the support is $[0,1]$.  Our target probability measure $\Bbb P$ is set to the uniform distribution on  $[0,1]$ (smoothly tapering to zero $0$ outside of $[0,1]$ for numerical convenience).
%At this sample size, we do not expect an accurate estimate of the sampling density, of course, but we can use this case to illustrate that by suitable selection of knots we can approximate the PMLE.
  For simplicity we choose the Gaussian kernel $R(x,y)=\exp\bigl(-\frac{(x-y)^2}{2\sigma^2}\bigr)$, with $\sigma=0.1$, to generate the RKHS $V$ and use the  penalty parameter $\lambda$ set to $10$. The top left plot in Figure~\ref{f2} shows the non-parametric density estimate $\hat f= e^{ H\circ \phi^{\hat v}_1}  |D\phi_1^{\hat v}| $ in red, generated by applying a gradient based optimization algorithm applied to the subclass (\ref{knots}) where the knots $\{\kappa_1,\ldots, \kappa_N\}=\{X_1,\ldots, X_n\}$ are kept fixed and the coefficients $\eta_1,\ldots, \eta_N$ are optimized by minimizing the penalized log likelihood function $E_\lambda (v)$ given in (\ref{energy1}).  
 To diagnose the richness of subclass (\ref{knots}) within the full Hilbert space we define the function $\mathcal D_t^v(x)$ for any $v\in V^{[0,1]}$ and any $t\in [0,1]$ as follows
 \begin{equation}
 \label{diag1}
 \mathcal D_t^v(x) \equiv    \frac{1}{ n}\sum_{k=1}^n \bigl[\beta_{k,t}^v\bigr]^T R(x, X_{k,t}^v )  +  \frac{1}{ n}\sum_{k=1}^n   \nabla_{y}^T R(x,y)\Bigr|_{y= X_{k,t}^v}
 \end{equation}
where $X_{k,t}^v\equiv \phi_t^{v} (X_k)$ and 
$ \beta_{k,t}^v\equiv   \nabla H(X^v_{k,1}) D\phi^{v}_{t1}(X_{k,t}^v)  +\nabla \log\det D\phi^{ v}_{t1} (X^v_{k,t})$. 
The function $\lambda v_t - \mathcal D_t^v$ serves as a diagnostic criterion in the sense that
  the Hilbert norm of $\lambda v_t - \mathcal D_t^v$ gives  the maximal rate of change of the penalized log-likelihood $E_\lambda(v)$, within the full infinite dimensional Hilbert space $V^{[0,1]}$. In particular, 
 \[   \biggl[\int_0^1\|\underbrace{ \lambda v_t-\mathcal D_t^v}_\text{\scriptsize diagnostic } \|_{V}^2dt\biggr]^{1/2} = \sup_\text{\small $\{ u\colon \| u \|_{V^{[0,1]}}= 1 \}$} \left[\frac{d}{d\epsilon} E_\lambda (v+\epsilon u)\right]_{\epsilon = 0} .   \]
 Therefore if   $\lambda v_t(x)-\mathcal D_t^v(x) = 0$ for all $t\in [0,1]$ and $x\in \Bbb R^d$, then $v$ satisfies the Euler-Lagrange equation. Discrepancies between $\lambda v_t(x)$ and $\mathcal D_t^v$ when optimizing over the subclass (\ref{knots}) indicates the subclass that is insufficient rich to reach the stationary points of $E_\lambda(v)$. 
 The diagnostic plots in this example, which correspond to our density estimate shown in the upper-left image of Figure~\ref{f2},  are shown in the upper-right plot of Figure~\ref{f2} where $\lambda v_0(x)$ is plotted in  black and $\mathcal D_0^v(x)$ is plotted as a dashed green line. The large amount of discrepancy between $\lambda v_0(x)$  and $\mathcal D_0^v(x)$ indicates that the knots $\{\kappa_1,\ldots, \kappa_N\}=\{ X_1,\ldots,X_n\}$  are insufficient. 
 
 

\begin{figure}[t]
\centering
\includegraphics[height = 2.2in]{pics/fig02/density_justn}
\includegraphics[height = 2.2in]{pics/fig02/agreement0_justn}\\
\includegraphics[height = 2.2in]{pics/fig02/density_subsample}
\includegraphics[height = 2.2in]{pics/fig02/agreement0_subsample}
\caption{ 
 \label{f3} 
 In this example we demonstrate that a small number of knots, in (\ref{knots}), can be enough to approximate solutions to the  Euler-Lagrange equation (\ref{ELeq}). The left column of images shows two different density estimates (red), based on the same data set  of size $n=240$ (grey histogram), using two different knot configurations (top-left uses $240$ knots, bottom-left uses $20$ knots). The right column of images show the corresponding  diagnostic curves which characterize the richness of the approximating subclass generated by the knots. The fact that the two diagnostic curves shown bottom-right are nearly identical suggests that the $20$ knots  used generate the approximating subclass by (\ref{knots}) is sufficiently rich to reach the stationary points of the penalized log-likelihood $E_\lambda$ given in (\ref{energy1}).
See Section \ref{npe} for details.
 }
 \end{figure}




To generate  knots which are sufficiently rich, in this first example,  we apply a discrete approximation at initial time $t=0$ to the gradient term $ \nabla_{y}^T R(x,y) $ appearing in  the Euler-Lagrange equation (\ref{ELeq}).
 For this approximation we use $N=3n$ knots in the pattern given by the following approximation
\begin{align}
 \hat v_0(x)&=  \frac{1}{\lambda n}\sum_{k=1}^n \beta^T_{k,0} R(x,X_{k})  +  \frac{1}{\lambda n}\sum_{k=1}^n   \nabla^T_{y} R(x,y)\Bigr|_{y= X_{k}} \\
 & \approx \frac{1}{\lambda n}\sum_{k=1}^n \beta^T_{k,0} R(x,\kappa_{k})  +  \frac{1}{\delta \lambda n}\sum_{k=1}^n   R(x,\kappa_{n+k})-R(x,\kappa_{2n+k}) \\
 & = \sum_{k=1}^N \eta^T_{k} R(x,\kappa_{k}) \label{eq:etakappa}
\end{align}
where 
$\kappa_{k}\equiv\begin{cases}
X_{k} & \text{if $k \in 1 \dots n$} \\
X_{k}+\frac{\delta}{2} & \text{if $k \in n+1 \dots 2n$} \\
X_{k}-\frac{\delta}{2} & \text{if $k \in 2n+1\dots 3n$} 
\end{cases}$ and $\eta_{k}\equiv\begin{cases}
\frac{1}{\lambda n} \beta_{k,0} & \text{if $k \in 1 \dots n$} \\
\frac{1}{\delta \lambda n} & \text{if $k \in n+1 \dots 2n$} \\
-\frac{1}{\delta \lambda n} & \text{if $k \in 2n+1 \dots 3n$} \\
\end{cases}$
with $\delta=10^{-4}$. 
%Illustrate the method by showing that the diagnostic curves can tell you when you have enough knots and talk about .
 With this new set of knots, the resulting PMLE over the new class is show at bottom left in Figure~\ref{f2}. Notice that now the diagnostic function $\lambda v_0 -\mathcal D_0^v$ (the difference between the black and green line in the bottom-right plot of Figure~\ref{f2}) is much closer to zero. Indeed, for every $t\in [0,1]$ the diagnostic function $\lambda v_t -\mathcal D_t^v$ is similarly close to zero (not pictured). This implies that the maximal rate of change  of the penalized log-likelihood within the infinite dimensional Hilbert space $V^{[0,1]}$, at our estimate, is very small and hence our knots are sufficiently rich.
 
 
   
 
In the previous example we used $N=3n$ knots in (\ref{knots}) to construct a sufficiently rich class for solving the Euler-Lagrange equation (\ref{ELeq}). Now we demonstrate that with larger data sets and smaller smoothness penalties one can actually use a smaller set of knots, $N\ll n$, to approximate the solutions to Euler-Lagrange equation (\ref{ELeq}). The histograms in the left column of Figure~\ref{f3} show $n=240$ {\em iid}  samples from the same truncated mixture of normals used in the previous example. The resulting density estimates, shown in red,  use a smoothness penalty set to $\lambda=1/4$. The estimate shown top-left  utilizes  $N=n=240$ knots set at the data points whereas the estimate shown bottom-left  uses $N=20$ knots randomly selected from the data.  The right column 
shows the corresponding diagnostic plots ($\lambda v_0$ shown in black  and $\mathcal D_0^v$ shown in green). The relative agreement of the diagnostic curves in the bottom-right plot suggests that 20 knots are reasonably adequate for finding approximate solutions to the Euler-Lagrange equation. We expect this situation to improve as the number of data points increase. This has the potential to dramatically decrease the computational load when applying this estimate to extremely large data sets. 






%%%%%%%%%%
% section
%%%%%%%%%%%%%%%
\section{Semiparametric example} 
\label{spe}

In this section we demonstrate how the PMLE $\phi_1^{\hat v}$ can be used to generate semiparametric estimation procedures obtained by assuming a parametric model on the target distribution $\Bbb P$ then introduce a nonparametric diffeomorphism to the target model. 
Indeed, any parametric model $\{\Bbb P_\theta\colon \theta\in \Theta\subset \Bbb R^m\}$ can  be extended to a   semiparametric class by considering diffeomorphisms of the data to the parametric target as follows: $\{\Bbb P_\theta \circ \phi^{v}_1 \colon \theta\in \Theta, v\in V^{[0,1]}\}$.
Since the  model $X_1,\ldots, X_n\overset{iid}\sim \Bbb P_\theta\circ \phi^v_1$ implies $\phi^v_1(X_1),\ldots, \phi_1^v(X_n)\overset{iid}\sim \Bbb P_\theta$ it is natural to alternate the optimization of $\theta$ and $\phi$ to compute the estimates $\hat \theta$ and $\hat \phi$ under this semiparametric model. This optimization routine is outlined explicitly in Algorithm \ref{alg2}.

\begin{algorithm}[h!]
\caption{Compute the semiparametric estimates $\hat\theta, \hat \phi$} 
\label{alg2} 
\begin{algorithmic}[1]
\STATE {{Set} $i=0$ and {initialize} $(\theta^0,\phi^0)$.}
\STATE { {Set}  $\phi^{i+1}$ to the PMLE of $\phi$ defined in Section \ref{pmle} under the model $X_1, \ldots, X_n \overset{iid}\sim \Bbb P_{\theta^i}\circ \phi$}.
\STATE {{Set} $\theta^{i+1}$ to the maximum likelihood estimate  of $\theta\in \Theta$ under the following model for the transformed data points:
\[ \phi^{i+1}(X_1), \ldots, \phi^{i+1}(X_n) \overset{iid}\sim \Bbb P_{\theta}\]}
\STATE {{If} $\theta^i\approx \theta^{i+1}$ and $\phi^i\approx \phi^{i+1}$ {then return} $(\hat\theta, \hat\phi)\leftarrow(\theta^{i+1},\phi^{i+1})$; {else
set} $i\leftarrow i+1$ and {return} to \mbox{step {\footnotesize 2}.}}
\end{algorithmic}
\end{algorithm}


\begin{figure}[t]
\centering
\includegraphics[height = 2.2in]{pics/fig04/normal_target_density}
\includegraphics[height = 2.2in]{pics/fig04/normal_target_agreement0}\\
\includegraphics[height = 2.2in]{pics/fig04/mixture_target_density}
\includegraphics[height = 2.2in]{pics/fig04/mixture_target_agreement0}
\caption{ 
In this example we demonstrate that by parametrically modeling the target distribution one can  produce flexible semiparametric density estimates. 
The left column of histograms show the data (the same histogram plotted twice) sampled from the population density shown in black. Two semiparametric estimates are shown in red which correspond to different parametric targets. The estimated target distribution is shown in green on the left column of images. The right column of images show the corresponding  diagnostic curves which characterize the richness of the approximating subclass generated by the knots ($\lambda v_0$ is plotted in  black and $\mathcal D_0^v$ is plotted in dashed-green).  See Section \ref{spe} for details. 
 \label{f4} }
 \end{figure}


To illustrate the semiparametric nature of our estimate we sample from a population density which is a mixture of a $\chi^2$ density (with $20$ degrees of freedom) and a Gaussian density ($\mu = 55$ and $\sigma = 3$) shown in black on the left column of plots in Figure \ref{f4}. The data  comprises $n=200$ independent samples from this mixture, the histogram of which is shown on the left column of plots in Figure \ref{f4}.
 We consider two different semiparametric estimates of the population density. The first uses a basic  location-scale Gaussian family to the parametric target model
 $\{ \Bbb P_\theta \colon \theta \in\Theta \} \equiv \{ \Bbb G_{\mu,\sigma} \colon   \mu \in \Bbb R,  \sigma \in \Bbb R^+ \}$ where $\Bbb G_{\mu,\sigma}$ denotes the Gaussian measure centered at $\mu$ with variance $\sigma^2$.
 The second example uses a
 mixture of two Gaussian measures $\{ \Bbb P_\theta \colon \theta \in\Theta \} \equiv \{ \alpha\,\Bbb G_{\mu_1,\sigma_1} +(1-\alpha)\,\Bbb G_{\mu_2,\sigma_2} \colon  \mu_1, \mu_2 \in \Bbb R,  \sigma_1,\sigma_2 \in \Bbb R^+,   0< \alpha < 1 \}$.
As in Section \ref{npe} we use a Gaussian reproducing kernel to generate the Hilbert space $V$. In this example, however, we use a wider kernel, with standard deviation set to half the sample standard deviation of the data.  This is done to illustrate the flexibility in the estimated density obtained by simply changing the kernel width and the penalty parameter $\lambda$ (which is decreased to $1/2500$ in this example).
Wider kernels tend to produce estimates which have restricted local variability but can still have sufficient flexibility to model large amplitude variations over large spatial scales.

The estimate obtained from the basic location-scale Gaussian family is shown in red in the top-left plot of Figure \ref{f4}. Conversely, the estimated density which uses the mixture target model is shown in red in the bottom-left plot of Figure \ref{f4}.   The corresponding estimated target density $d\Bbb P_{\hat \theta}/dx$ is shown in green on the left two plots.
To numerically approximate the PMLE initial velocity field $\hat v_0$, needed in step 2 of Algorithm \ref{alg2},  we used the approximating subclass for the initial velocity field given in the form (\ref{knots}) with $200$ knots located at the data values.
The corresponding time zero diagnostic plots are shown in the right column of Figure \ref{f4} (green for $\lambda v_0$ and black for $\mathcal D_0^v$).  
 Notice that in both cases the semiparametric estimates do a good job at estimating the population density. 
In the case of the location-scale Gaussian target the estimated target density does a poor job of explaining the true density. However, the presence of a nonparametric diffeomorphism allows this model to fit nearly as well as a fit from a mixture model. Notice also that the semiparametric estimate based on the location-scale Gaussian target overestimates the true sampling density between the two modes. This seems due to the fact that the estimation procedure prefers an overly dispersed target density which  allows the estimated diffeomorphism to effectively add  mass around the smaller mode. The situation seem to be corrected when using a mixture. 





%%%%%%%%%%%%%%%%%%%
\section{Discussion}

%The use of smooth invertible transformations or diffeomorphisms are fast becoming important tools in modern data analysis. For example, time varying vector field flows which generate a class of diffeomorphisms  are being used with spectacular success in the computational anatomy literature  (see \cite{you:10} and the references therein). In the field of spatial statistics, on the other hand, diffeomorphisms are used to model nonstationary  random fields with a locally varying geometric anisotropy \cite{Sampson:1992fk}. %In the field of cosmology, diffeomorphisms model the lensing distortion of background images from the gravitational effect of dark matter (see \cite{Dod} for example).
% In this paper, we use diffeomorphisms to recast the problem of density estimation to that of  diffeomorphism estimation, extending an idea  from \cite{and:11}  to general dimension $\Bbb R^d$.  The basic structure of our methodology is to fix a target probability measure then estimate a diffeomorphism which pushes forward the sampling distribution to the target measure. The resulting PMLE density estimate can generate nonparametric and semiparametric density estimates while  also incorporating  shape information for the unknown sampling distribution.
In this paper,  we adapt the powerful tools developed by Grenander, Miller,  Younes, Trouv\'e and co-authors in the image processing and computational anatomy literature   (see \cite{you:10} and the references therein) to generate a PMLE of a deformation to a target measure with all the required properties: smoothness, invertability and computational tractability.
The two main theoretical contributions of this paper are found in Claim \ref{claim1}  and Claim \ref{claim2}. Claim 1 establishes the existence of the diffeomorphism PMLE  over a Hilbert space generating the initial velocity fields which give rise to the geodesic diffeomorphisms. Claim 2 shows that the PMLE  has a finite dimensional  spline characterization  when the initial velocities are restricted to reproducing kernel Hilbert spaces. 
This finite dimensional characterization, although similar in spirit to spline function estimation, is completely different in that it holds for the initial velocity fields which generate the geodesic diffeomorphisms. 
  A secondary contribution of this paper is the realization that the Euler-Lagrange equation for the PMLE also allows one to construct a diagnostic for approximating sub-models of the initial velocity field which are more amenable to computation. This diagnostic can be used to test whether a sub-model is sufficiently rich to reach the stationary points of the penalized maximum likelihood over the  full infinite dimensional Hilbert space.  This has the potential for significant computational savings when applying the estimate to large data sets in high dimension.
In Section \ref{steinSection} we make an explicit connection between the Euler-Lagrange equation for the PMLE and Stein's method for bounding distances between two probability measures. 
This connection is used to motivate and explain the Euler-Lagrange equation and also hints at a new approach for deriving a theoretical understanding of the PMLE.
The paper concludes with  two illustrative examples which are not intended to be a compete simulation analysis but instead to illustrate the estimate and give a hint at the potential applicability of this new methodology. Indeed, the flexibility provided by both the diffeomorphism and the target measure make the methodology potentially applicable to a wide range of problems: 
from manifold estimation to density estimation on manifolds and from nonparametric measures of goodness of fit to
 heteroscedastic regression.

%Indeed, there are many more directions of future research and we hope the results in this paper spur development into this area. 
%Clearly the theoretical development of the estimate is a natural direction. Also, for the estimate to become widely applicable there also needs to be a study of adaptive choices of smoothing penalty $\lambda$. Of course, cross validation is the natural first choice but others may be equally competatives.  In addition, we have only given brief glimpse at the possibilities for the target model which can possibly be extended to density estimates on manifolds   manifolds and more general shape constrained models.
% In general one can also use regression target models, then the diffeomorphism scold potentially model heterogsigsatiscisy or left out interactions. 
%We close by mentioning that The connection with Stein's method is particularly enticing as  a potential way to further the theoretical understanding of the estimate.



\appendix



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Details}
\label{TD}


This section serves to present some technical details which are used  in the proofs of Claim \ref{claim1} and Claim \ref{claim2}.   Some of these results can be found in the current literature. In particular,  Proposition \ref{Hspace}, most of Proposition \ref{PropDef} and equation (\ref{111}) can be found in \cite{you:10}.   However, the main goal of this section is to establish  equation (\ref{888}) in Proposition \ref{proo}  which is key to establishing Claim \ref{claim2}. 
We mention that all of the derivations presented in this section rely heavily on techniques developed by  Younes, Tr\'ouve, Miller and co-authors (see \cite{you:10} and references therein).



%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline

\begin{definition}
Here are some basic definitions and basic facts. Let $\Omega \subset \Bbb R^d$ be an open set. 
\begin{enumerate}
\item $C^0(\Omega)$ is the set of continuous functions on $\Omega$.
\item $C^0(\bar\Omega)$ is the set of continuous functions on $\bar\Omega$.
\item $C^k(\Omega)$ is the set of functions have all derivatives of order $\leq k$ continuous in $\Omega$.
\item $C^k(\bar\Omega)$ is the set of functions in $C^k(\Omega)$ all of whose derivatives of order $\leq k$ have continuous extensions to $\bar\Omega$.
\item The {\em support } of a function $f$ is the closure of the set on which $f\neq 0$.
%\item $C_0^k(\Omega)$ is the set of functions in $C^k(\Omega)$ with compact support in $\Omega$. \textcolor{blue}{Note:} this seems to differ a bit from the defintion in Younes who says that $C_0^k(\Omega)$ is the set of functions in $C^k(\Omega)$ which vanish on  $\partial\Omega$.
\item A complete normed linear space is called a {\em Banach space}.
\item For $f\in C^k(\bar\Omega)$ set 
\[ \| f \|_{C^k(\bar \Omega)}\equiv \| f \|_{k,\infty}\equiv \sum_{j=0}^k \sup_{|\beta|=j}\sup_{\Omega} |D^\beta f|.\]
\item If $f\in C^\alpha(\bar\Omega)$ and $g\in C^\beta(\bar\Omega)$ then $fg \in C^{\beta\wedge \alpha}(\bar\Omega)$ and 
\[ \|fg  \|_{\beta\wedge \alpha,\infty}\leq c \|f  \|_{ \alpha,\infty}\|g  \|_{\beta,\infty}  \]
where $c$ only depends on $d,\alpha, \beta$.
\item  $C^k(\bar\Omega)$ are Banach spaces with respect to the norm $\| \cdot \|_{k,\infty}$.
\end{enumerate}
\end{definition}

{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%------------------------------ end long version

To set notation  let $C^k(\Omega, \Bbb R^d)$ denote the set of functions, mapping an open set $\Omega\subset \Bbb R^d$ into $\Bbb R^d$, which have continuous derivatives of order $\leq k$ (so that $C^0(\Omega, \Bbb R^d)$ is the continuous functions on $\Omega$ mapping into $\Bbb R^d$).
Also let $C^k(\bar\Omega,\Bbb R^d)$ denote the set of functions in $C^k(\Omega,\Bbb R^d)$ whose derivatives of order $\leq k$ have continuous extensions to $\bar\Omega$. Finally, $C_0^k(\Omega,\Bbb R^d)$ is the set of functions in $C^k(\bar\Omega,\Bbb R^d)$ whose derivatives of order $\leq k$ take the value $0$ on the boundary $\partial \Omega$. It is a well known fact that  $C^k(\bar\Omega,\Bbb R^d)$ is a Banach space with respect to the norm:  $ \| f \|_{C^k(\bar \Omega)}\equiv \| f \|_{k,\infty}\equiv \sum_{j=0}^k \sup_{|\beta|=j}\sup_{\Omega} |D^\beta f|$.



{\em Remark:} The norm $\| v\|_V\equiv \int_0^1 \| v_t\|^2_V dt $ given in Definition \ref{defV01} is technically only a semi-norm since one is free to change $v_t$  on a set of $t\in[0,1]$ with Lebesque measure zero (and not effect the norm on $V^{[0,1]}$). This is easily fixed by identifying  $V^{[0,1]}$ with the set of equivalence classes of measurable functions where $\{v_t\}_{t\in [0,1]}$ and $\{w_t\}_{t\in [0,1]}$ are said to be in the same equivalence class if $\| v_t-w_t \|_V=0$ for almost every $t\in [0,1]$. For the remainder of the paper we treat this identification as implicit with the understanding that $\{v_t\}_{t\in[0,1]}$  denotes a representer of the equivalence class to which is belongs. The following proposition establishes the Hilbert space structure of $V^{[0,1]}$ (stated without proof in 8.17 of \cite{you:10}).

\begin{proposition}
\label{Hspace}
If $V$ is a Hilbert space with inner product $\langle\cdot, \cdot \rangle_V$, then
$V^{[0,1]}$ is a Hilbert space with inner product  defined by $\langle v,h \rangle_{V^{[0,1]}} \equiv\int_0^1 \langle  v_t, h_t\rangle_V dt$.
\end{proposition}

\begin{proof}
Notice first that $\| v_t\|_V$ and $\langle  v_t, h_t\rangle_V$ are measurable functions of $t$ (this can be taken to be implicit in definitional requirement for membership in $V^{[0,1]}$: that $\int_0^1 \| v_t \|^2_Vdt <\infty$). Now, with the exception of completeness, all the properties of a Hilbert space inner product are inherited from $\langle  \cdot , \cdot\rangle_V$ and the linear properties of Lebseque integration over $[0,1]$. 
To show completeness  let $v^n\in V^{[0,1]}$ be a Cauchy sequence so that $\int_0^1 \| v_t^m - v_t^n \|^2_V dt\rightarrow 0$. By the completeness of $V$ there exists a Borel set $B\subset[0,1]$ such that for all $t\in B$ there exists a $v_t\in V$ such that $\|v_t^n - v_t\|_V\rightarrow 0$. On $t\in [0,1]\setminus B$ we are free to set $v_t\equiv 0$ (the zero element of $V$). For this $v_t$ we have that $\|v^n_t - v_t \|_{V^{[0,1]}}^2\equiv \int_0^1 \| v_t^n - v_t \|^2_V dt\rightarrow 0$. Therefore $V^{[0,1]}$ is complete.
\end{proof}



%%%%%%%%%%%%%%%%%
%begin  proposition
%%%%%%%%%%%%%%%%%%
\begin{proposition} 
\label{PropDef} Let $\Omega$ be an open bounded subset of $\Bbb R^d$ and $V$ be a Hilbert space such that  $V\hookrightarrow C_0^1(\Omega,\Bbb R^d)$. If   $v\in V^{[0,1]}$ then there exists a unique  class of $C^1$ diffeomorphisms of $\Omega$,  $\{\phi_{t}^v\}_{t\in [0,1]}$, such that  $\phi_t^v(x)\in C^0 ([0,1]\times \overline \Omega,\Bbb R^d)$ and  which satisfy the ordinary differential equation $
 \partial_t \phi_t^v(x) = v_t(\phi_t^v(x)) $ with boundary condition $\phi_0^v(x)=x$, for all $x\in \Omega$. 
Moreover,
 \begin{equation}
 \label{div}
  \log\det D\phi_{st}^v(x)  = \int_s^t  \text{\rm div}\,  v_u (\phi_{su}^v(x))du. 
  \end{equation}
 \end{proposition}
%%%%%%%%%%%%%%%%
%begin proof
%%%%%%%%%%%%
\begin{proof} 
%
%%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline

We first show that there exists a $\delta>0$ and a solution $\phi^v_t(x)$   to the ordinary differential equation $ \partial_t \phi_t^v(x) = v_t(\phi_t^v(x)) $ on $(t,x)\in [0,\delta]\times\Omega$  with boundary condition $\phi_0^v(x)=x$ (for all $x\in \Omega$) which is in $C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$.  For any map $\varphi_t(x)\in C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$ we define the operator $\Gamma(\varphi)\equiv x + \int_0^t v_s(\varphi_s(x))ds $ (extending $v_s$ to $0$ beyond $\Omega$ if necessary). Notice  $\Gamma$ maps $C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$ into $C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$  since 
\begin{align*}
|x + \int_0^t v_s(\varphi_s(x))ds  - y - \int_0^w v_s(\varphi_s(y))ds |
&\leq |x-y| + \int_t^w | v_s(\phi_s(x))-v_s(\varphi_s(y))  |ds \\
&\leq |x-y| + c\int_t^w \| v_s \|_V|\phi_s(x)-\varphi_s(y)  |ds \\
&\rightarrow 0, \quad\text{by DCT as $t\rightarrow w$ and $x\rightarrow y$}
\end{align*}
and also by the fact that $|x + \int_0^t v_s(\varphi_s(x))ds| \leq \sup_{x\in \Omega}|x| +  c\int_0^1 \|v_s\|_V ds <\infty$.
We show that $\delta$ can be chosen to make $\Gamma$ a contraction with respect to sup norm on $C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$
\begin{align}
\|\Gamma(\varphi) - \Gamma(\varphi^\prime) \|_{\infty} &\leq \int_0^t |v_s(\varphi_s(x))-  v_s(\varphi^\prime_s(x)) |ds \nonumber\\
&\leq \int_0^t c\|v_s\|_V|\varphi_s(x)-  \varphi^\prime_s(x) |ds\nonumber \\
&\leq \|\varphi-  \varphi^\prime \|_\infty  \int_0^\delta c\|v_s\|_V ds \label{Contract}
\end{align}
If we choose $\delta$ so that $\int_0^\delta c\|v_s\|_V ds < \gamma<1$ then the above inequality shows that $\Gamma$ is contraction operator on the Banach space $C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$. In fact, for reasons that will become obvious later we notice that since $\int_0^{t} c\| v_s \|_Vds$ is uniformly continuous for all $t\in[0,1]$ we can choose a $\delta$ such that $c\int_{t}^{t+\delta} \|v_s\|_V ds < \gamma<1$ uniformly over all $t\in [0,1]$. Therefore there exists a unique fixed point $\phi_t(x) \in C^0 ([0,\delta]\times \overline \Omega,\Bbb R^d)$ which satisfies
\[ \phi_t(x) = x + \int_0^t v_s(\phi_s(x))ds. \]
Now to extend  the definition of $\phi_t(x)$ to $ t\in [0,2\delta]$ notice one can repeat the same argument to get the existence of $\phi_{\delta t}(x)$ which satisfies $\partial_t \phi_{\delta t}(x) = v_t(\phi_{\delta t}(x)) $ on $(t,x)\in [\delta,2\delta]\times\Omega$  with boundary condition $\phi_{\delta \delta}(x)=\phi_\delta(x)$.
In particular simply set 
\begin{equation}
\phi_t^v(x)\equiv \begin{cases}
\phi_t(x) & \text{when $t\in[0,\delta]$}\\
\phi_{\delta t}(x) & \text{when $t\in[\delta,2\delta]$}
\end{cases}
\end{equation}
Now repeat the argument over successive intervals $[k\delta, (k+1) \delta]$. Notice that this can lead to kinks at the endpoints $k\delta$, but this can be fixed by working with overlapping intervals and using the uniqueness of the fixed point.
This method then produces a unique   $\phi_t^v(x)\in C^0 ([0,1]\times \overline \Omega,\Bbb R^d)$ and  which satisfy the ordinary differential equation $
 \partial_t \phi_t^v(x) = v_t(\phi_t^v(x)) $ with boundary condition $\phi_0^v(x)=x$, for all $x\in \Omega$. 


Now we need to show that for all $t\in [0,1]$, $\phi_t$ is a $C^1$ diffeomorphism of $\Omega$. 

{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version

First note that if $v\in V^{[0,1]}$ and $V\hookrightarrow C_0^1(\Omega, \Bbb R^d)$ then $ \| v_t \|_{1,\infty} \leq c \| v_t \|_{V} $. Now by H\"older, $\int_0^1 \| v_t \|_{V} dt \leq \|v \|_{V^{[0,1]}}<\infty$  so that the arguments for  Theorem 8.7   in \cite{you:10} to apply to the class  $ V^{[0,1]}$. In particular, there exists a unique  class of $C^1$ diffeomorphisms of $\Omega$, $\phi_t^v(x)\in C^0 ([0,1]\times \overline \Omega,\Bbb R^d)$, which satisfy the ordinary differential equation $\partial_t \phi_t^v(x) = v_t(\phi_t^v(x))$
 with boundary condition $\phi_0^v(x)=x$, for all $x\in \Omega$. Moreover, by Proposition 8.8 in \cite{you:10} we have that
 \begin{align}
    \partial_t D\phi_{st}^v(x)  =  D v_t (\phi_{st}^v(x))  D\phi_{st}^v(x) \label{gg}
\end{align}
where $\det D\phi_{ss}^v(x)=Id_d$. 
Since $D\phi_{st}^v(x)$ is nonsingular and differentiable in $t$ we have that (see  (6.5.53) of \cite{hor:91}, for example)
\begin{align*}
\partial_t \log \det D\phi_{st}^v(x)&= \text{trace}\bigl\{ [D\phi_{st}^v(x)]^{-1}   \partial_t D\phi_{st}^v(x)  \bigr\} \\
& = \text{trace}\bigl\{ [D\phi_{st}^v(x)]^{-1}  D v_t (\phi_{st}^v(x))  D\phi_{st}^v(x) \bigr\} ,\,\,\text{by (\ref{gg})} \\
& = \text{trace}\bigl\{ D v_t (\phi_{st}^v(x))  \bigr\} \\
& = \text{div}\, v_t (\phi_{st}^v(x)).
\end{align*}
Therefore $\log \det D\phi_{st}^v(x)$ is differentiable everywhere on $t\in [0,1]$ with derivative given by $ \text{div}\, v_t (\phi_{st}^v(x))$.  

Since $v_t(x)$ is measurable with respect to both arguments $t$ and $x$ (by definition) and limits of measurable functions are measurable, the function $\text{div}\,v_t(x)$  is also measurable. Since $\phi^v_{st}(x)$ is continuous with respect to both $t$ and $x$,  $\text{div}\, v_t (\phi_{st}^v(x))$ is also measurable. 
Notice that $\text{div}\, v_t (\phi_{st}^v(x))$  is also Lebesque integrable since $|\text{div}\, v_t (\phi_{st}^v(x))| \leq c\| v_t \|_V$ by the embedding $V\hookrightarrow C_0^1(\Omega,\Bbb R^d)$ and the fact that that  $\int_0^1 \| v_t \|_{V} dt \leq \|v \|_{V^{[0,1]}}<\infty$.
%. Therefore
%\begin{align*}
 %\int_0^1|\text{div}\, v_t (\phi_{st}^v(x))| dt &\leq   \int_0^1 \| v_t \|_{1,\infty}  \leq  c  \| v \|_{V^{[0,1]}} <\infty.
 %\end{align*} 
% Therefore $\text{\rm div}\,v_t(x)$ is indeed Lebesque integrable as was to be shown.
Therefore by Theorem 7.21 of \cite{rud:66} we have that
\[\log \det D\phi_{st}^v(x) =  \int_s^t  \text{div}\, v_u (\phi_{su}^v(x))  du\]
since $\log \det D\phi_{ss}^v(x) = 0$. \end{proof}




%%%%%%%%%
% begin lemma
\begin{lemma} If $V\hookrightarrow C_0^1(\Omega,\Bbb R^d)$ and $v,w\in V^{[0,1]}$, then
\begin{align}
% \|\phi_{st}^v\|_\infty &\leq \sup_{x\in \Omega} |x| \label{firstEE} \\
 \|\phi_{st}^v - \phi_{st}^w    \|_{\infty}& \leq c \| v-w \|_\text{\tiny $V^{[0,1]}$} \exp{\left( c\| v \|_\text{\tiny $V^{[0,1]}$} \right)}
  \label{222}
\end{align}
where $c$ is a constant which does not depend on $v, w, s$ or $t$.
Moreover, if we additionally suppose $V\hookrightarrow C_0^2(\Omega, \Bbb R^d)$ then
\begin{align}
  \label{777}
\| \phi_{st}^v - \phi_{st}^w \|_{1,\infty}\leq \| v-w \|_\text{\tiny $V^{[0,1]}$}  F\bigl( {\|v\|_\text{\tiny $V^{[0,1]}$}},{\|w\|_\text{\tiny $V^{[0,1]}$}}\bigr)
\end{align}
where $F(\cdot,\cdot)$ is  a finite  function on $\Bbb R\times \Bbb R$, monotonically increasing in both arguments, which does not depend on $v, w, s$ or $t$. 
\end{lemma}
%%%%%%%%
% end lemma
%%%%%%%%%%


%%%%%%%%%%%%
% begin proof
%%%%%%%%%%%%%%%%%
\begin{proof}
The inequality (\ref{222})  follows directly from Grownwell's lemma  applied to the following inequality
\begin{align*}
|\phi_{st}^v(x) - \phi_{st}^w(x)| &= \left| \int_{s}^t v_u (\phi_{su}^v(x)) -  w_u (\phi_{su}^w(x)) du\right|\\
&\leq \int_{s}^t |v_u (\phi_{su}^v(x)) -  v_u (\phi_{su}^w(x)) |du + \int_{s}^t |v_u (\phi_{su}^w(x)) -  w_u (\phi_{su}^w(x)) |du\\
%& \leq   \int_{s}^t \|v_u\|_{1,\infty} |\phi_{su}^v(x) -  \phi_{su}^w(x)| du  + \int_0^1 \|  v_u - w_u \|_{\infty} du \\ 
& \leq \int_{s}^t c\|v_u\|_{V} |\phi_{su}^v(x) -  \phi_{su}^w(x)|du  +  c\|v - w  \|_{V^{[0,1]}}
\end{align*}
where the last inequality follows from the assumption $V\hookrightarrow C_0^1(\Omega, \Bbb R^d)$.
%%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
 Therefore 
 \[  |\phi_{st}^v(x) - \phi_{st}^w(x)| \leq  c_2\|v - w  \|_{V^{[0,1]}} \exp\left( \int_{s}^t \|v_u\|_{1,\infty}  du \right) \]
 by Grownwell's lemma.  This proves (\ref{222}).
 {\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version



 
 
 To prove (\ref{777}) notice that for any vector $h\in \Bbb R^d$ we have that $\partial_t D \phi_{st}^v(x)h= Dv_t (\phi_{st}^v(x))D \phi_{st}^v(x)h $ where $ D \phi_{ss}^v(x)h = h$ (by Proposition 8.8 in \cite{you:10} and also  \cite{dup:98}). Therefore
 \begin{align}
 \label{uugg}
 D \phi_{st}^v(x)h -  D \phi_{st}^w(x)h = \int_s^t \Bigl[  Dv_u (\phi_{su}^v(x))D \phi_{su}^v(x)h -  Dw_u (\phi_{su}^w(x))D \phi_{su}^w(x)h \Bigr]\,du
 \end{align}
 where we are using the fact that $D \phi_{st}^v(x)h$  is differentiable with respect to $t$ everywhere in $[0,1]$ and with  Lebseque integrable derivative   (and using Theorem 8.21 of \cite{rud:66}). 
 Now notice that the integrand of (\ref{uugg}) satisfies
  \begin{align*}
\bigl| &Dv_u (\phi_{su}^v(x))D \phi_{su}^v(x)h -  Dw_u (\phi_{su}^w(x))D \phi_{su}^w(x)h  \bigr| \leq I + I\!I
\end{align*}
where 
\begin{align*}
I &=  \left| Dv_u (\phi_{su}^v(x)) \Bigl\{ D \phi_{su}^v(x)h - D \phi_{su}^w(x)h \Bigr\}  \right| \leq   c\bigl\| v_u\bigr\|_{V} \Bigl|   D \phi_{su}^v(x)h - D \phi_{su}^w(x)h  \Bigr| 
\end{align*}
and
\begin{align*}
I\!I & =  \left|\Bigl\{ Dv_u (\phi_{su}^v(x)) -  Dw_u (\phi_{su}^w(x))  \Bigr\} D \phi_{su}^w(x)h  \right| \\
&\leq   \Bigl\{ \| v_u \|_{2,\infty} \| \phi_{su}^v - \phi_{su}^w \|_\infty   + \bigl\| v_u-w_u\bigr \|_{1,\infty} \Bigr\} \,  \bigl\| \phi_{su}^w\bigr\|_{1,\infty} \, |h |\\
&\leq \Bigl\{   c \| v_u \|_{V^{\phantom{[}}}  \|v-w\|_{V^{[0,1]}} \exp\left(c\| w \|_{V^{[0,1]}} \right)   + c\bigl\| v_u-w_u\bigr \|_{V} \Bigr\}\,  \bigl\| \phi_{su}^w\bigr\|_{1,\infty} \, |h |
%-----
 \end{align*}
 %%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
We are using the fact that for any $v_t\in V$ we have that $\bigl |Dv_t (x) h \bigr |\leq \| v_t \|_{1,\infty } | h| $ for any $ h,  x \in \Bbb R^d$.  Also we are using the fact that for any $v_t\in V$ we have that $\bigl |(Dv_t (x) - Dv_t(y))h \bigr |\leq \| v_t \|_{2,\infty }|x-y| | h| $ for any $ h,  x \in \Bbb R^d$.  
  {\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version
where the last inequality follows by (\ref{222}). To bound $I\!I$ further notice $
  \|\phi_{st}^v\|_{1,\infty} \leq c_1\exp{\left( c\| v \|_\text{\tiny $V^{[0,1]}$} \right)}$. To see why,
apply Gronwall's lemma to the following inequality
\begin{align*}
|\phi_{st}^v(x) - \phi_{st}^v(y)| &= \left| x-y + \int_{s}^t v_u (\phi_{su}^v(x)) -  v_u (\phi_{su}^v(y)) du\right|\\
& \leq  |x-y| + \int_{s}^t c\, \|v_u\|_{V^{\phantom{[}}} |\phi_{su}^v(x) -  \phi_{su}^v(y)| du
\end{align*}
which yields $| \phi_{st}^v(x) - \phi_{st}^v(y) |\leq |x-y| \exp{\left( c\| v \|_\text{\tiny $V^{[0,1]}$} \right)}$.
  Since $\phi_{st}^v(x)$ is differentiable with respect to $x$ everywhere in $\Omega$, for each multi-index $\beta$ such that $|\beta|=1$ there exists a direction $h^\beta\in \Bbb R^d$ (with $|h^\beta|=1$) such that 
 \begin{align*}
 |D^\beta \phi_{st}^v(x) | &= \lim _{\epsilon \downarrow 0} \frac{|\phi_{st}^v(x+\epsilon h^\beta) - \phi_{st}^v(x)|}{\epsilon}  \leq  \exp{\left( c\| v \|_\text{\tiny $V^{[0,1]}$} \right)}.
 \end{align*} 
 Combining the above inequality with the fact that $ \|\phi_{st}^v\|_\infty \leq \sup_{x\in \Omega} |x| $ gives  the desired inequality 
 $ \|\phi_{st}^v\|_{1,\infty} \leq c_1\exp{\left( c\,\| v \|_\text{\tiny $V^{[0,1]}$} \right)}$. Applying this to $ I\!I $ gives
 \begin{align*}
 I\!I &\leq \Bigl\{  c \| v_u \|_{V}  \|v-w\|_{V^{[0,1]}} \exp\left(c \| w \|_{V^{[0,1]}} \right)   + c \bigl\| v_u-w_u\bigr \|_{V} \Bigr\}\, c_1\exp{\left( c \| w \|_\text{\tiny $V^{[0,1]}$} \right)} |h|
 \end{align*}
 Therefore
 \begin{align*}
\int_s^t I\!I\, du 
&\leq c_1 c |h| \|v-w \|_{V^{[0,1]}}\Bigl\{\|v\|_{V^{[0,1]}} \exp\bigl( 2c\| w \|_{V^{[0,1]}} \bigr)  +  \exp\bigl( c\| w \|_{V^{[0,1]}} \bigr)  \Bigr\}\\
%&\leq  c_1 |h|  \|v-w\|_{V^{[0,1]}} \exp\left(c_3\| w \|_{V^{[0,1]}} \right) \Bigl[ c_5 \| v \|_{V^{[0,1]}}  \exp\left(c_2\| w \|_{V^{[0,1]}} \right)   + c_4  \Bigr] \\
& =  |h|  \|v-w\|_{V^{[0,1]}}  F\left(\|v \|_{V^{[0,1]}} ,\| w\|_{V^{[0,1]}}\right) 
\end{align*}
where $F(x,y)$ is monotone and finite in both $x$ and $y$. Now by equation (\ref{uugg}) we have that 
\begin{align*} 
\bigl|D \phi_{st}^v(x)h -  D \phi_{st}^w(x)h\bigr | & \leq \int_s^t I du + \int_s^t I\!I du \\
&\leq  \int_s^t c\bigl\| v_u\bigl\|_{V} \bigl|   D \phi_{su}^v(x)h - D \phi_{su}^w(x)h  \bigr| du \\
&\qquad\qquad +  |h|  \|v-w\|_{V^{[0,1]}}  F\bigl(\|v \|_{V^{[0,1]}},\| w\|_{V^{[0,1]}}\bigr).
\end{align*}
By Gronwell's lemma we have that
\[\bigl|D \phi_{st}^v(x)h -  D \phi_{st}^w(x)h\bigr | \leq  |h|  \|v-w\|_{V^{[0,1]}}  F\bigl(\|v \|_{V^{[0,1]}},\| w\|_{V^{[0,1]}}\bigr) \exp\bigl ( c \bigl\| v\bigl\|_{V^{[0,1]}} \bigr) . \]
Now by taking a supremum over $x\in \Omega$, $|h|=1$ and combining with (\ref{222}) gives (\ref{777}), after redefining $F$ to accommodate the extra term $\exp ( c \bigl\| v\bigl\|_{V^{[0,1]}} )$.


%Therefore by Gronwall's lemma we have that 
%$ |\phi_{st}^v(x) - \phi_{st}^v(y)|\leq |x-y| \exp\left( \int_{s}^t   \|v_u\|_{1,\infty} du \right)$ which establishes  (\ref{555}). 

 


\end{proof}
%%%%%%%%%%%%
%  end proof
%%%%%%%%%%%%%%%

%%%%%%%
% begin lemma
%%%%%%%%%
\begin{proposition} 
\label{proo}
If $v,h \in V^{[0,1]}$ and $V\hookrightarrow C_0^1(\Omega,\Bbb R^d)$ then for all $x\in \Omega$ and $s,t\in [0,1]$ 
\begin{align}
{\partial_\epsilon}  \phi^{ v+\epsilon h}_{st}(x)  &= \int_s^t  \bigl\{D\phi^{ v+\epsilon h}_{ut} h_u \bigr\}\circ{\phi^{ v+\epsilon h}_{su}(x)}\,   du. \label{111}
 % & =   \int_0^1 [D\phi_1(x)]  [D\phi_t(x)]^{-1} h_t ( \phi^{\hat v+\epsilon h}_{t}(x))   dt. 
 \end{align}
 %%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
If, in addition, $V\hookrightarrow C_0^2(\Omega,\Bbb R^d)$ then ${\partial_\epsilon}  \phi^{ v+\epsilon h}_t(x)  $ is locally Lipschitz continuous in $\epsilon$   (with respect to sup-norm over $x\in \Omega$). 
   {\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version
If, in addition,  $V\hookrightarrow C_0^3(\Omega,\Bbb R^d)$ then
\begin{align}
  \label{888}
\partial_\epsilon  \log \det D\phi_{1}^{v+\epsilon h }(x)\bigr|_{\epsilon = 0} & = \int_0^1  \Bigl[ h_u \cdot \nabla \log\det D\phi_{u1}^v  + \text{\rm div}\, h_u\Bigr] \circ  \phi^v_{u}(x)  \, du
\end{align}
\end{proposition}




%%%%%%%%%%%%
% begin proof
\begin{proof} The assumption that $v,h \in V^{[0,1]}$ and $V\hookrightarrow C_0^1(\Omega,\Bbb R^d)$ are sufficient to apply
Theorem 8.10 of \cite{you:10} which gives
\begin{align*}
{\partial_\epsilon}  \phi^{ v+\epsilon h}_{st}(x) \bigr|_{\epsilon = 0} &= \int_s^t  \bigl\{D\phi^{ v}_{ut} h_u \bigr\}\circ{\phi^{ v}_{su}(x)}\,   du. 
 % & =   \int_0^1 [D\phi_1(x)]  [D\phi_t(x)]^{-1} h_t ( \phi^{\hat v+\epsilon h}_{t}(x))   dt. 
 \end{align*}
 Now since ${\partial_\epsilon}  \phi^{ v+\epsilon h}_{st}(x) = {\partial_\xi}  \phi^{ v+\epsilon h+\xi h}_{st}(x)\bigr|_{\xi = 0} $ one immediately obtains (\ref{111}). 
 
 %%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline

    Now we can prove the local Lipschitz continuity of  ${\partial_\epsilon}  \phi^{ v+\epsilon h}_t  $ in $\epsilon$  (with respect to sup-norm). Let $\xi, \epsilon\in (-M, M)$ for some $0<M<\infty$ and fix $v,h\in V^{[0,1]}$ and write
\begin{align}
\left \| \partial_\epsilon  \phi^{ v+\epsilon h}_{st} -  \partial_\xi   \phi^{ v+\xi h}_{st}  \right\|_{\infty} & = \left \| \int_s^t  \bigl\{D\phi^{ v+\epsilon h}_{ut} h_u \bigr\}\circ{\phi^{v+\epsilon h}_{su}(x)}-  \bigl\{D\phi^{v+\xi h}_{ut} h_u \bigr\}\circ{\phi^{v+\xi h}_{su}(x)}\,   du    \right\|_{\infty}\nonumber \\
& \leq  \left\| \int_s^t  \bigl\{D\phi^{ v+\epsilon h}_{ut} h_u \bigr\}\circ{\phi^{ v+\epsilon h}_{su}(x)}-  \bigl\{D\phi^{ v+\xi h }_{ut} h_u \bigr\}\circ{\phi^{v+\epsilon h}_{su}(x)}\,   du    \right\|_{\infty}\label{hhhh1}  \\ 
& \qquad+ \left\| \int_s^t  \bigl\{D\phi^{v+\xi h}_{ut} h_u \bigr\}\circ{\phi^{v+\epsilon h}_{su}(x)}-  \bigl\{D\phi^{v+\xi h}_{ut} h_u \bigr\}\circ{\phi^{ v+\xi h}_{su}(x)}\,   du    \right\|_{\infty}  \label{hhhh2}
\end{align}
The integrand of the first  term (\ref{hhhh1}) can be bounded by noticing that   $V\hookrightarrow C_0^{2}(\Omega, \Bbb R^d)$ implies
\begin{align}
  \Bigl| \bigl\{D\phi^{ v+\epsilon h}_{ut} h_u \bigr\}\circ{\phi^{ v+\epsilon h}_{su}(x)}- & \bigl\{D\phi^{ v+\xi h }_{ut} h_u \bigr\}\circ{\phi^{v+\epsilon h}_{su}(x)}\Bigr| \nonumber\\
  &\leq \|  (D\phi_{ut}^{ v+\xi h} -D \phi_{ut}^{ v+\epsilon h})h_u   \|_\infty  \nonumber\\
  &\leq  \|  \phi_{ut}^{ v+\xi h} - \phi_{ut}^{v+\epsilon h}  \|_{1,\infty}  \| h_u\|_{\infty} \nonumber\\
&\leq |\xi -\epsilon| \|  h  \|_{V^{[0,1]}}  F\bigl( {\|v+\xi h\|_\text{\tiny $V^{[0,1]}$}},{\|v+\epsilon h\|_\text{\tiny $V^{[0,1]}$}}\bigr)   \| h_u\|_{V},\,\,\text{by (\ref{777})} \nonumber\\
&\leq c_1 |\xi -\epsilon|   \| h_u\|_{V}
\label{uuuu}
\end{align}
where $c_1$ is a finite constant which depends on $v$ and $h$ but not on   $\xi$ or $\epsilon$.
The integrand of the second term  (\ref{hhhh2}) can be bounded as follows:
\begin{align}
 \Bigr|\bigl\{D\phi^{v+\xi h}_{ut} h_u \bigr\}\circ{\phi^{v+\epsilon h}_{su}(x)}-  &\bigl\{D\phi^{v+\xi h}_{ut} h_u \bigr\}\circ{\phi^{ v+\xi h}_{su}(x)}\Bigl| \nonumber   \\
  &\leq   \bigl\| D\phi^{v+\xi h}_{ut} h_u \bigr\|_{1,\infty} \bigl| \phi^{v+\epsilon h}_{su}(x)-  \phi^{ v+\xi h}_{su}(x)  \bigr| \nonumber   \\
 &\leq  c_2 |\xi -\epsilon|  \| D\phi^{v+\xi h}_{ut} h_u \|_{1,\infty} ,\,\,\text{by (\ref{222})} \nonumber   \\
&\leq  c_2 |\xi -\epsilon|   \| h_u \|_{1,\infty} \| \phi^{v+\xi h}_{ut} \|_{2,\infty}  \nonumber   \\
&\leq  c_2 c_3|\xi -\epsilon|   \| h_u \|_{V}   \label{fine} 
\end{align}
where $c_2, c_3$  are finite constants which depends on $v$ and $h$ but not on   $\xi$ or $\epsilon$ (the existence of $c_3$ follows again from  Theorem 8.9 of \cite{you:10}). Combining (\ref{uuuu}) and (\ref{fine}) with (\ref{hhhh1}) and (\ref{hhhh2}) shows that $ \partial_\epsilon  \phi^{ v+\epsilon h}_{st}$  is locally Lipschitz in $\epsilon$.

   {\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version

 
 
 
 To show (\ref{888}) notice  that partial derivatives on $x$ can pass under the integral in (\ref{111})  to compute $D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{1}$. This follows by first noticing that  $V\hookrightarrow C_0^{2}(\Omega, \Bbb R^d)$ implies
  \begin{align}
  \sup_{u\in[0,1]} \bigl\| \phi_{ut}^{v+\epsilon h} \bigr\|_{2,\infty}&\leq c_1 \exp\left({c_2 \|  {v\|_{V^{[0,1]}}+M\| h} \|_{V^{[0,1]}}}\right) \label{811}
  \end{align}
  for all $|\epsilon|< M$, by equation (8.11) of \cite{you:10}.
  %%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
Here is the above inequality with an extra step.
 \begin{align}
  \sup_{u\in[0,1]} \| \phi_{ut}^{v+\epsilon h} \|_{p+1,\infty}&\leq c_1 \exp\left({c_2 \|  {v+\epsilon h} \|_{V^{[0,1]}}}\right)\\
  &\leq c_1 \exp\left({c_2 \|  {v\|_{V^{[0,1]}}+M\| h} \|_{V^{[0,1]}}}\right) <\infty \label{811}
  \end{align}
 Note that   (8.11) of  \cite{you:10} can be proved using
 \begin{align}
 \label{rrmark}
  \| \phi_{st}^v(x) - \phi_{ss}^v(x) \|_{p,\infty}
  &\leq\int_s^t \|  v_u(\phi_{su}^v(x))   \|_{p,\infty} du
 \end{align}
 and then applying Proposition 8.4 of \cite{you:10} along with Gronwall's identity and induction. Notice that when $p\geq 1$ equation (\ref{rrmark}) can be obtained, not from the identity $\phi_{st}^v(x) - \phi_{ss}^v(x)= \int_s^t  v_u(\phi_{su}^v(x))    du$, but from the identity
 \[ D^\beta\phi_{st}^v(x) - D^\beta\phi_{ss}^v(x)= \int_s^t    D^\beta(v_u(\phi_{su}^v(x)))   du.\]
 which follows from Proposition 8.8 of  \cite{you:10}.
   {\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%%------------------------------ end long version
Therefore when fixing $v, h\in V^{[0,1]}$ the function $ \| D\phi^{v+\epsilon h}_{ut} h_u \|_{1,\infty}$ is bounded above by a finite constant over $(u,\epsilon)\in[0,1]\times (-M,M)$. With an additional application of Proposition 8.4 of \cite{you:10} we also have that
  $ \bigl\| \{D\phi^{v+\epsilon h}_{ut} h_u\}\circ \phi_{su}^{v+\epsilon h} \bigr\|_{1,\infty}<\infty$ uniformly over $(u,\epsilon)\in[0,1]\times (-M,M)$.
Therefore, indeed, partial derivatives on $x$ can pass under the integral in (\ref{111})  to obtain
\begin{equation}
\label{pass}
D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{1}(x) = \int_0^1 D\bigl[ \{D\phi_{u1}^{v+\epsilon h} h_u  \}\circ \phi_u^{v+\epsilon h} (x)\bigr]  du. 
\end{equation}

Now we show that $D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{1}(x)$ is continuous  over $(x,\epsilon)\in \Omega\times (-M,M)$. This will allow us to switch the order of $D$ and $\partial_\epsilon$ and establish (\ref{888}). The same reasoning which allows $D$ to pass under the integral in (\ref{111})  also allows us to pass limits on $x$ and $\epsilon$ under the integral in (\ref{pass}). Therefore it will be sufficient to show the integrand, $ D\bigl[ \{D\phi_{u1}^{v+\epsilon h} h_u  \}\circ \phi_u^{v+\epsilon h}(x)  \bigr]$,  in (\ref{pass}) is continuous  over $(x,\epsilon)\in \Omega\times (-M,M)$.
To see why   the integrand in (\ref{pass})  is continuous first note that $ \phi^{ v+\epsilon h}_{st}$ is a $C^2$ diffeomorphism (by a similar proof Theorem 8.7 in \cite{you:10}). Secondly, under the assumption $V\hookrightarrow C_0^3(\Omega, \Bbb R^d)$ one can extend (\ref{777}) to bound  $\| \phi_{st}^{v+\epsilon h} - \phi_{st}^{v+\xi h} \|_{2,\infty}$ by $c|\xi - \epsilon|$, where $c$ is a finite constant which may depend on $v,h$ but not on $\xi, \epsilon$. These two facts imply that $ D\bigl[ \{D\phi_{u1}^{v+\epsilon h} h_u  \}\circ \phi_u^{v+\epsilon h}(x)  \bigr]$ is indeed continuous in $(x,\epsilon)\in \Omega\times (-M,M)$ which implies that $D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{1}(x)$ is also. 

The continuity of $D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{1}(x)$ over $(x,\epsilon)\in \Omega\times (-M,M)$ implies  $ \partial_\epsilon D \phi^{ v+\epsilon h}_{st}$ exists and   $D {\partial_\epsilon}  \phi^{ v+\epsilon h}_{st}  =  {\partial_\epsilon} D \phi^{ v+\epsilon h}_{st}$ (see \cite{cou:36}, page 56). Then since $D\phi_{st}^{v+\epsilon h}$ is nonsingular (by the diffeomorphic property) and differentiable  with respect to $\epsilon$ we have that
\begin{align*}
\partial_\epsilon  \log \det D\phi_{st}^{v+\epsilon h }(x)&= \text{trace}\bigl\{ [D\phi_{st}^{v+\epsilon h}(x)]^{-1}   \partial_\epsilon D\phi_{st}^{v+\epsilon h}(x) \bigr\} \\
 &= \text{trace}\bigl\{ [D\phi_{st}^{v+\epsilon h}(x)]^{-1}  D \partial_\epsilon \phi_{st}^{v+\epsilon h}(x)  \bigr\} .
\end{align*}
Therefore, by (\ref{pass}),
\begin{align*}
\partial_\epsilon  \log \det D\phi_{1}^{v+\epsilon h }(x)\bigr|_{\epsilon = 0} %& =\text{trace}\left\{  \bigl[ D\phi_1^v(x) \bigr]^{-1} \int_0^1 D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_u^v(x)  \bigr]  du  \right\} \\
%& =\text{trace}\left\{  \bigl[ D\phi_1^v(x) \bigr]^{-1} \int_0^1 D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_u^v\circ \phi_1^{-1}\circ \phi_1(x)  \bigr]  du  \right\} \\
& =\text{trace}\left\{  \bigl[ D\phi_1^v(x) \bigr]^{-1} \int_0^1 D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_{1u}^v\circ \phi^v_1(x)  \bigr]  du  \right\} \\
& =\text{trace}\left\{  \bigl[ D\phi_1^v(x) \bigr]^{-1} \int_0^1 D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_{1u}^v(y)\bigr]\Bigr|_{y=\phi^v_1(x)}     du\, D\phi^v_1(x)  \right\} \\
%& = \int_0^1\text{trace}\left\{  D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_u^v\circ \phi_1^{-1}(y)\bigr]\Bigr|_{y=\phi^v_1(x)}   \right\}    du   \\
& = \int_0^1\text{trace}\left\{  D\bigl[ \{D\phi_{u1}^v h_u  \}\circ \phi_{1u}^v(y)\bigr]\Bigr|_{y=\phi^v_1(x)}   \right\}    du .  
\end{align*}
Now notice that
\begin{align*}
\text{trace}\, D \bigl[ \{  D\phi_{u1}^v h_u\}\circ \phi^v_{1u}\bigr] 
&= \text{trace}\, \bigl[ \{  D (D\phi_{u1}^v h_u)\}\circ \phi^v_{1u} D(\phi^v_{1u})\bigr] \\
&= \text{trace}\, \bigl[    \{D  (D\phi_{u1}^v  h_u) \} (D\phi_{u1}^v )^{-1}\bigr]\circ \phi^v_{1u} \\
 &=\bigl\langle  h_u\circ  \phi^v_{1u} ,(\nabla \log\det D\phi_{u1}^v )\circ  \phi^v_{1u} \bigr\rangle_d + (\text{div}\, h_u) \circ  \phi^v_{1u} .
\end{align*}
The last line follows from the identity: $\text{trace}\, \bigl[    \{D  [D\phi_{u1}^v h_u] \} (D\phi_{u1}^v)^{-1}\bigr]= \bigl\langle  h_u,\nabla \log\det D\phi_{u1}^v \bigr\rangle_d + \text{trace}(D h_u)$. 
%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
$\text{trace}\, \bigl[    \{D  D\phi h \} (D\phi)^{-1}\bigr]= \bigl\langle  h,\nabla \log\det D\phi \bigr\rangle_d + \text{div}\, h$. This is easily seen to be true using a symbolic mathematical program. For example the following {\sc{Matlab}} code works
\begin{quote}
\tt{syms x y;\\
f1=sym('f1(x,y)');\\
f2=sym('f2(x,y)'); \\
h1=sym('h1(x,y)'); \\
h2=sym('h2(x,y)'); \\
F=[f1;f2]; \\
h=[h1;h2]; \\
DF=jacobian(F,[x y]);\\
LHS=trace( jacobian(DF*h,[x y])*inv(DF) );\\
RHS=(jacobian(log(det(DF)),[x y])  )*h + trace(jacobian(h,[x y]));\\
simplify(LHS-RHS)\\
\%Note that this simplifies to zero!!!!!}
\end{quote}
{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%------------------------------ end long version
Therefore
 \begin{align*}
\partial_\epsilon  \log \det D\phi_{1}^{v+\epsilon h }(x)\bigr|_{\epsilon = 0} & = \int_0^1  \Bigl[ h_u \cdot \nabla \log\det D\phi_{u1}^v   + \text{div}\, h_u  \Bigr]\circ  \phi^v_{u}(x)  du.
\end{align*}
 
 
\end{proof}
%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline
%%%%%%%%%%%%%%%
\section{Radial kernel derivatives}
The following lemma shows how to simplify the evolution of $q,m, A$ and $b$ in the above equations when the reproducing kernel $R(x,y)$ is a radial kernel $R(|x-y|)$.
\begin{lemma}
Let $R$ be a real valued function defined on $[0,\infty)$ such that  $R(|\cdot|)\in C^4(\Bbb R)$. If $x\neq y$ are in $\Bbb R^d$, then
\begin{align}
\nabla_x R(|x-y|)&=R^\prime(|x-y|)\frac{(x-y)}{|x-y|} \label{yy1}\\
\nabla_y \otimes \nabla_x R(|x-y|)&=(x-y)\otimes (y-x) \left[ \frac{R^{\prime\prime}(|x-y|)}{|x-y|^2}  -  \frac{R^{\prime}(|x-y|)}{|x-y|^3} \right]  - \frac{R^{\prime}(|x-y|) }{|x-y|} \text{\rm Id}_d  \label{yy2} \\
\nabla_{y} [\nabla_y \cdot \nabla_x R(|x-y|)]&= \frac{y-x}{|x-y|}\left[-R^{\prime\prime\prime}(|x-y|)+ (1-d)\frac{R^{\prime\prime}(|x-y|)}{|x-y|} -   (1-d)\frac{R^{\prime}(|x-y|)}{|x-y|^2}  \right].  \label{yy3}
\end{align}
When $x=y$,
\begin{align}
\nabla_x R(|x-y|)\Bigr|_{x=y}&=0   \label{yy4}\\
\nabla_y \otimes \nabla_x R(|x-y|)\Bigr|_{x=y}&= -\text{\rm Id}_d  R^{\prime\prime}(0)  \label{yy5}\\
\nabla_{y} [\nabla_y \cdot \nabla_x R(|x-y|)]\Bigr|_{x=y}&=0.    \label{yy6}
\end{align}
Finally notice that $\nabla_y \otimes \nabla_x R(|x-y|)=-\nabla_x \otimes \nabla_x R(|x-y|)$.
\end{lemma}

%%%%%%%%%%%%%%%
\begin{proof}
The equation (\ref{yy1}) holds by noticing that $\nabla_x |x-y|= (x-y)/|x-y|$. Then equation  (\ref{yy4}) holds since $R^{\prime}(|x-y|)\rightarrow 0$ as $|x-y|\rightarrow 0$. The second equation (\ref{yy2}) follows since
\[ \nabla_y \otimes \nabla_x R(|x-y|)=  \nabla_y  [(x-y) F(|x-y|)]=-e_i F(|x-y|)+ F^\prime(|x-y|) \frac{(y-x)\otimes (x-y)}{|x-y|}\]
where $F(y)=R^\prime(y)/y$ and $e_i=(0,\ldots,1,\ldots,0)$ (with the $1$ in the $i^\text{th}$ coordinate) so that
\[ F^\prime(y)=\frac{R^{\prime\prime}(y)y- R^{\prime}(y)}{y^2} =\frac{R^{\prime\prime}(y)}{y}-\frac{ R^{\prime}(y)}{y^2}.\]
Also notice that $ F^\prime(|x-y|) \frac{(y-x)\otimes (x-y)}{|x-y|}\rightarrow 0$ as $|x-y|\rightarrow 0$ since each coordinate is bounded by $F^\prime(|x-y|) |x-y|= R^{\prime\prime}(|x-y|)- R^\prime(|x-y|)/|x-y|\rightarrow R^{\prime\prime}(0)- R^{\prime\prime}(0)=0$. Now since $F(|x-y|)\rightarrow R^{\prime\prime}(0)$ as $|x-y|\rightarrow 0$, we also get equation (\ref{yy5}).


Finally notice that (\ref{yy3}) follows since
\[ \nabla_y \cdot \nabla_x R(|x-y|)=\text{trace}\bigl\{\nabla_y \otimes \nabla_x R(|x-y|) \bigr\}=-R^{\prime\prime}(|x-y|)+(1-d)\frac{R^\prime(|x-y|)}{|x-y|}\]
Now when $x\neq y$
\[ \nabla_y [ \nabla_y \cdot \nabla_x R(|x-y|)]=-R^{\prime\prime\prime}(|x-y|)\frac{y-x}{|x-y|}+(1-d) F^\prime(|x-y|) \frac{y-x}{|x-y|}. \]
To study the case when $x=y$ we can form the difference quotient to compute  $\frac{\partial }{\partial y_i} \nabla_y \cdot \nabla_x R(|x-y|)$
\begin{align}
\left[\frac{\partial }{\partial y_i} \nabla_y \cdot \nabla_x R(|x-y|)\right]_{x-y=0}&=\frac{ \nabla_y \cdot \nabla_x R(|\epsilon e_i|)-\nabla_y \cdot \nabla_x R(0) }{\epsilon} \\
&=\frac{-R^{\prime\prime}(\epsilon)+(1-d){R^\prime(\epsilon)}/{\epsilon}  +d R^{\prime\prime}(0) }{\epsilon} \\
&=\frac{-R^{\prime\prime}(0)+ O(\epsilon^2)+(1-d){R^{\prime\prime}(0)} + O(\epsilon^2)  +d R^{\prime\prime}(0) }{\epsilon}\label{IamSmall} \\
&=O(\epsilon)
\end{align}
where (\ref{IamSmall}) following since $R^\prime(0)=R^{\prime\prime\prime}(0)=0$.
\end{proof}
{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%------------------------------ end long version


%%------------------------------ begin long version
\if\Ver\LongVer{ 
{\flushleft\textcolor{blue}{$\downarrow$---------begin long version---------}}\newline


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Geodesic flows
%%%%%%%%%%%%%%
\section{Numerical techniques: geodesic and transpose flows}
Here we explicitly show how to generate Geodesic flows.



%%%%%%%%%%%%%%%%%
\subsection{Geodesic flow  when  $v^t(x)=\sum_{j=1}^N \eta^t_j R(x,\kappa^t_j) $ on $\Bbb R^1$}
Suppose $v^t(x)$ is a vector field on $\Bbb R$ such that $v^t(x)=\sum_{j=1}^N \eta^t_j R(x,\kappa^t_j)$.
We give formal arguments that  show how to generate the full path of coefficients $\{\eta^t_j: t\in [0,1], j=1,\ldots, N\}$ and knots $\{\kappa^t_j: t\in [0,1], j=1,\ldots, N\}$ from the $t=0$ values when the vector field flow minimizes $\frac{1}{2}\int_0^1 \| v^t  \|^2_R dt$ under the endpoint constraints $\kappa_j^{0}=X_j$ and $\kappa_j^{1}=\phi^1(X_k)$. {\em Imagine restricting all the flows to having this form, then minimizing the PMLE. The minimum fixes $\phi_\text{optim}^1$ and the flow vector field also minimizes  $\frac{1}{2}\int_0^1 \| v^t  \|^2_R dt$ under the endpoint constraints $\kappa_j^{0}=X_j$ and $\kappa_j^{1}=\phi_\text{optim}^1(X_k)$}

Start by noticing that 
\begin{align*}
\frac{1}{2}\int_0^1 \| v^t  \|^2_V dt &=  \frac{1}{2}\int_0^1  \langle \sum_{i=1}^N R(\kappa_i,\cdot ) \eta_i, \sum_{j=1}^N R(\kappa_j,\cdot ) \eta_j\rangle dt \\
&= \frac{1}{2}\int_0^1 \sum_{i=1}^N \sum_{j=1}^N \eta_i^T \eta_j R(\kappa_i,\kappa_j) dt 
\end{align*}
where $\eta\equiv (\eta_1,\ldots,\eta_N)^T$ and $R=(R(\kappa_i,\kappa_j))_{i,j = 1}^N$ (where we are suppressing the time variable $t$ dependence on $\eta_j$ and $x_j$). Notice that for $i=1,\ldots, N$ we have $\dot \kappa_i = v(\kappa_i)= \sum_{j=1}^N R(\kappa_i,\kappa_j) \eta_j=\sum_{j=1}^N R_{i,j} \eta_j =: R\eta$ where $\kappa = (\kappa_1,\ldots, \kappa_N)^T$.
In particular, if $\ell\in \{ 1,2,\ldots, d \}$ then $\dot \kappa_{i,\ell} =\sum_{j=1}^N R(\kappa_i,\kappa_j) \eta_{j,\ell} =  R_{i,:} \eta_{:,\ell}$. Therefore $ R^{-1} \dot \kappa_{:,\ell} =\eta_{:,\ell}$.
\[ \frac{1}{2}\int_0^1 \| v^t  \|^2_V dt =  \frac{1}{2}\int_0^1 \sum_{i,j=1}^N (\dot\kappa_i^T\dot\kappa_j) \bigl(R^{-1}\bigr)_{i,j} \, dt  =  \frac{1}{2}\int_0^1 \dot\kappa^T R^{-1} \dot\kappa \, dt  = \int_0^1 L(\kappa,\dot\kappa) dt   \]
where $L(\kappa,\dot\kappa) =\dot\kappa^T R^{-1} \dot\kappa/2= \sum_{i,j=1}^N \dot\kappa_j^T\dot\kappa_j \bigl(R^{-1}\bigr)_{i,j} /2$.
 Now take a time varying perturbation $\kappa^t +\epsilon h^t$ which does not perturb the endpoints. In particular $h^0(\kappa^0_k)=h^1(\kappa^1_k)=0$ for all $k=1,\ldots, N$. Now formally since vector field flow minimizes $\frac{1}{2}\int_0^1 \| v^t  \|^2_R dt$ under the endpoint constraints $\kappa_j^{0}=X_j$ and $\kappa_j^{1}=\phi_1(X_k)$ we get 
\begin{align*}
0&=\frac{d}{d\epsilon} \left[\int_0^1 L(\kappa+\epsilon h,\dot \kappa+\epsilon \dot h)dt\right]_{\epsilon = 0} 
%&=\int_0^1\frac{d}{d\epsilon} \Bigl[ L(\kappa+\epsilon h,\dot \kappa+\epsilon \dot h)\Bigr]_{\epsilon = 0} dt\\
=\int_0^1\sum_{i=1}^N \Bigl[ \nabla_{\kappa_i}L(\kappa,\dot \kappa)\cdot h_i + \nabla_{\dot\kappa_i}L(\kappa,\dot \kappa)\cdot \dot h_i\Bigr]  dt  \\
&=\int_0^1\sum_{i=1}^N  \left[ \nabla_{\kappa_i} L(\kappa,\dot \kappa) - \frac{d}{dt} \nabla_{\dot\kappa_i}L (\kappa,\dot \kappa)\right] \cdot h_i  \, dt 
\end{align*}
where the last line follows by integration by parts and the assumption that $h$ is zero at the endpoints. Since the perturbation direction $h$ was arbitrary (with the endpoint constraints) we get Euler-Lagrange equation
\begin{equation}
\label{EL}
 \frac{d}{dt}\nabla_{\dot\kappa_i} L(\kappa,\dot \kappa) =  \nabla_{\kappa_i} L(\kappa,\dot \kappa)
\end{equation}
for all $i=1,\ldots, N$.
In less compact notation for each knot $i=1,\ldots, N$ and coordinate $\ell = 1,\ldots, d$
\begin{equation}
\label{EL}
 \frac{d}{dt}\bigl[\nabla_{\dot\kappa_i} L(\kappa,\dot \kappa)\bigr]_\ell = \bigl[ \nabla_{\kappa_i} L(\kappa,\dot \kappa)\bigr]_{\ell}
\end{equation}

Now, notice  $ \nabla_{\dot\kappa} L(\kappa,\dot \kappa) = R^{-1}\dot \kappa = \eta$ and the $i^\text{th}$ value of $\nabla_{\kappa} L(\kappa,\dot \kappa) $ can be computed as follows
\begin{align*}
\bigl[\nabla_{\kappa_i} L(\kappa,\dot \kappa)]_{\ell} & = \frac{\partial}{ \partial \kappa_{i,\ell}} \frac{ \dot\kappa^T R^{-1} \dot\kappa}{2}  = \sum_{p=1}^d \left[-\frac{1}{2} \dot\kappa^T_{:,p} R^{-1} \frac{\partial R}{\partial \kappa_{i,\ell}}  R^{-1} \dot\kappa_{:,p} \right] = \sum_{p=1}^d  \left[-\frac{1}{2} \eta_{:,p}^T \frac{\partial R}{\partial \kappa_{i,\ell}}  \eta_{:,p} \right]\\
&= \sum_{p=1}^d \sum_{j=1}^N \left[-\frac{\eta_{i,p} \eta_{j,p}}{2} \frac{\partial }{\partial \kappa_{i,\ell}}R(\kappa_i,\kappa_j)  -\frac{\eta_{i,p} \eta_{j,p }}{2}  \frac{\partial }{\partial \kappa_{i,\ell}}R(\kappa_j,\kappa_i)   \right]\\
&= -\sum_{j=1}^N ({\eta_i}^T \eta_j) \frac{\partial }{\partial \kappa_{i,\ell}}R(\kappa_i,\kappa_j) 
\end{align*}
where the last line follows when $R(x,y) = R(|x-y|)$ so that $R^{(1,0)}(x,y)= - R^{(0,1)}(x,y)$, $R^{(1,0)}(x,y)= - R^{(1,0)}(y,x)$ and $R^{(1,0)}(x,y)=  R^{(0,1)}(y,x)$.
Therefore we can simplify the Euler-Lagrange equation (\ref{EL}) to the following update equation for $\eta$
\begin{equation}
\label{updateEta} 
\frac{d\eta_i}{dt} = - \sum_{j=1}^N \underbrace{({\eta_i}^T\eta_j)}_{1\times 1}\underbrace{ \nabla^T_{\kappa_i}R(\kappa_i,\kappa_j)}_{d\times 1}. \end{equation}
Notice that we consider $\eta_i$ and $\kappa_i$ as $d\times 1$ column vectors and $\nabla_{\kappa_i}$ as a $1\times d$ row vector.
We also trivially have the update equation for the knots
\begin{equation}
\label{updateKnots}
\frac{d\kappa_i}{dt}=\sum_{j=1}^N \eta_j \underbrace{R(\kappa_i,\kappa_j)}_{1\times 1} .
\end{equation}
At the map level one gets, for any fixed $x$
\begin{equation}
\frac{d\phi(x)}{dt} = \sum_{j=1}^N \eta_j R(\phi(x),\kappa_j)
\end{equation}
 (note that $\phi(x)$ takes values in $\Bbb R^d$).
Taking a gradient with respect to $x$ gives:
\begin{equation}
\frac{d\nabla_x\phi(x)}{dt} = \sum_{j=1}^N \underbrace{\eta_j}_{d\times 1} \underbrace{\{ \nabla_{\phi(x)}R(\phi(x),\kappa_j)\}}_{1\times d} \underbrace{ \nabla_x {\phi(x)}}_{d\times d}
\end{equation}


%%%%%%%
\subsection{Perturbing $\eta$ and $\kappa$ at $t=0$}
Notice that since the full path of $\eta$ and $\kappa$ is determined at time $t=0$ one can perturb these values at time zero  $\eta^0+\epsilon \delta \eta^0$ and $\kappa^0+ \epsilon \delta \kappa^0$. If $\epsilon$ is infinitesimal, this results in perturbations $\delta \kappa$, $\delta \eta$ at all times. 
Taking the derivatives (with respect to $\epsilon$) on both sides of (\ref{updateEta}) and (\ref{updateKnots}) one gets the following linear ODE characterization of $\delta\eta$
%\begin{align}
%\frac{d\delta \eta_i}{dt} &= -\sum_{j=1}^N \delta\eta_i \,  \eta_j R^{(1,0)}(\kappa_i,\kappa_j) +  {\delta\eta_j}\,  \eta_i R^{(1,0)}(\kappa_i,\kappa_j) \nonumber \\
%&\qquad - \sum_{j=1}^N \delta\kappa_i\, {\eta_i}  \eta_j R^{(2,0)}(\kappa_i,\kappa_j)+ \delta\kappa_j\, {\eta_i}  \eta_j R^{(1,1)}(\kappa_i,\kappa_j).
%\end{align} 
\begin{align}
\frac{d\delta \eta_i}{dt} = -&\sum_{j=1}^N \bigl[ \delta\eta^T_i \eta_j +\eta^T_i \delta\eta_j    \bigr] \nabla^T_{\kappa_i}R(\kappa_i,\kappa_j) \\
&-\sum_{j=1}^N \eta^T_i \eta_j  \bigl[  \{\nabla^T_{\kappa_i}\nabla_{\kappa_i}R(\kappa_i,\kappa_j)\} \delta\kappa_i +  \underbrace{ \{\nabla^T_{\kappa_i}\nabla_{\kappa_j}R(\kappa_i,\kappa_j)\} }_{d\times d}\delta\kappa_j \bigr].
\end{align}
Similarly one gets the following linear ODE characterization of $\delta\kappa$
\begin{align}
\frac{d\delta\kappa_i}{dt}=&\sum_{j=1}^N \delta\eta_j\, R(\kappa_i,\kappa_j) + \eta_j \bigl[ \{\nabla_{\kappa_i} R(\kappa_i,\kappa_j)\} \delta\kappa_i + \{\nabla_{\kappa_j} R(\kappa_i,\kappa_j)\} \delta\kappa_j   \bigr].
\end{align}
At the map level one also gets perturbations $\delta \phi_\ell$ and $\delta \nabla\phi_\ell$ (where $\phi_\ell := \phi(X_\ell)$ and $\nabla\phi_\ell := \nabla\phi(X_\ell)$ and $\ell=1,\ldots, n$) with initial conditions $\delta \phi_\ell^0 \equiv 0$ and  $\delta \nabla \phi^0_\ell \equiv 0$. 
To get these notice
\begin{align}
\frac{d\delta\phi_\ell}{dt} &= \sum_{j=1}^N \delta\eta_j R(\phi_\ell,\kappa_j) +\eta_j\bigl[ \{\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\} \delta\phi_\ell   +  \{\nabla_{\kappa_j}R(\phi_\ell,\kappa_j)\} \delta\kappa_j  \bigr]
\end{align}
Finally we get
\begin{align}
\frac{d\delta\nabla\phi_\ell}{dt} 
&= \sum_{j=1}^N  \delta\eta_j  \{ \nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\} \nabla{\phi}_\ell   \\
&+ \sum_{j=1}^N \eta_j \bigl\{ \underbrace{\delta\phi^T_\ell}_{1\times d} \underbrace{[\nabla_{\phi_\ell}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)]}_{d\times d} +   \delta\kappa_j^T [\nabla_{\kappa_j}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)]     \bigr\} \underbrace{\nabla {\phi_\ell}}_{d\times d}  \\
&+ \sum_{j=1}^N \eta_j \{ \nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\} \delta\nabla {\phi}_\ell
\end{align}

\subsection{Simplify the ode flow for $(\delta \eta,\delta \kappa, \delta \phi, \delta \nabla \phi)$}
\label{simplifiedODE}
Let's simplify the above ode's to expose the linear structure.

Here is the equation for $\delta \eta_i$:
\begin{align*}
\frac{d\delta \eta_i}{dt} &= \sum_{j=1}^N \underbrace{ \bigl[ \nabla^T_{\kappa_i}R(\kappa_i,\kappa_j)\bigr]}_{d\times 1} \underbrace{\bigl[-  \eta_j^T   \bigr]}_{1\times d}\delta\eta_i  \\
&+ \sum_{j=1}^N  \bigl[\nabla^T_{\kappa_i}R(\kappa_i,\kappa_j)\bigr] \bigl[- \eta^T_i     \bigr]\delta\eta_j\\
&+\sum_{j=1}^N \bigl[-\eta^T_i \eta_j\bigr]  \bigl[  \nabla^T_{\kappa_i}\nabla_{\kappa_i}R(\kappa_i,\kappa_j)  \bigr]\delta\kappa_i \\
&+\sum_{j=1}^N {\bigl[-\eta^T_i \eta_j\bigr]}  \bigl[   \underbrace{ \{\nabla^T_{\kappa_i}\nabla_{\kappa_j}R(\kappa_i,\kappa_j)\} }_{d\times d}\bigr]\delta\kappa_j .
\end{align*}


Here is the equation for $\delta \kappa_i$:
\begin{align*}
\frac{d\delta\kappa_i}{dt}&=\sum_{j=1}^N  \bigl[R(\kappa_i,\kappa_j)\bigr] \delta\eta_j  \\
&+\sum_{j=1}^N \underbrace{\bigl[\eta_j\bigr]}_{d\times 1} \underbrace{\bigl[ \nabla_{\kappa_i} R(\kappa_i,\kappa_j)\bigr]}_{1\times d} \delta\kappa_i \\
&+\sum_{j=1}^N \bigl[\eta_j\bigr] \bigl[ \nabla_{\kappa_j} R(\kappa_i,\kappa_j) \bigr] \delta\kappa_j.
\end{align*}
Here is the equation for $\delta \phi_\ell$:
\begin{align}
\frac{d\delta\phi_\ell}{dt} &= \sum_{j=1}^N \bigl[R(\phi_\ell,\kappa_j)\bigr] \delta\eta_j  \\
&+ \sum_{j=1}^N \underbrace{\bigl[\eta_j\bigr]}_{d\times 1} \underbrace{\bigl[ \nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{1\times d}  \delta\phi_\ell \\
& + \sum_{j=1}^N \bigl[\eta_j\bigr]\bigl[   \nabla_{\kappa_j}R(\phi_\ell,\kappa_j) \bigr] \delta\kappa_j .
\end{align}

We can write the matrix $\nabla \phi^t(X_\ell)$ as $\bigl[\partial_{x_1}\phi^t(X_\ell), \ldots,\partial_{x_d}\phi^t(X_\ell )\bigr]$ where for each $p=1,\ldots, d$ the quantity $\partial_{x_p}\phi^t(X_\ell)$ is a column vector. Therefore
\begin{align*}
\frac{d\delta\partial_{x_p}\phi_\ell}{dt} 
&= \sum_{j=1}^N\underbrace{\bigl[ \nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{1\times d} \underbrace{\bigl[\partial_{x_p}{\phi}_\ell \bigr]}_{d \times 1}  \delta\eta_j \\
&+ \sum_{j=1}^N \underbrace{\bigl[ \eta_j\bigr]}_{d\times 1} \underbrace{\bigl[\partial_{x_p} {\phi_\ell^T}\bigr]}_{1\times d} \underbrace{\bigl[\nabla_{\phi_\ell}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{d\times d}  \delta\phi_\ell  \\
&+ \sum_{j=1}^N \underbrace{\bigl[\eta_j\bigr]}_{d\times 1}   \underbrace{\bigl[\partial_{x_p} {\phi_\ell^T}\bigr]}_{1\times d}  \underbrace{\bigl[\nabla^T_{\phi_\ell}\nabla_{\kappa_j}R(\phi_\ell,\kappa_j)\bigr]}_{d\times d}  \delta\kappa_j    \\
&+ \sum_{j=1}^N \underbrace{\bigl[\eta_j\bigr]}_{d\times 1} \underbrace{\bigl[ \nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{1\times d} \delta\partial_{x_p} {\phi}_\ell
\end{align*}


%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\subsection{Transpose flow when  $v^t(x)=\sum_{j=1}^N \eta^t_j R(x,\kappa^t_j) $ on $\Bbb R^1$}

The previous equations give the forward flow
\begin{equation}
\frac{d}{dt} \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta \nabla\phi^t   \end{array}\right]
= U_t  \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\nabla\phi^t   \end{array}\right]
\end{equation}
Now we need to derive the transpose flow
\begin{equation}
\label{simSys}
\frac{d}{dt} \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta \nabla\phi^t   \end{array}\right]
= -U_t^T  \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\nabla\phi^t   \end{array}\right]
\end{equation}
where $U_t$ is a matrix which depends on $\eta^t_i, \kappa^t_i, \phi^t(X_\ell) $ and $\nabla\phi^t(X_\ell)$ but not on  $\delta\eta^t, \delta\kappa^t, \delta\phi^t(X_\ell), \delta \nabla\phi^t(X_\ell)$. 
Before we derive the transpose flow lets simplify the system (\ref{simSys}). Notice that by examining the equation given in Section \ref{simplifiedODE} we get the following block for of the matrix $U_{t}$
\begin{equation}
\frac{d}{dt} \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\partial_{x_1} \phi^t  \\ \vdots \\ \delta\partial_{x_d} \phi^t   \end{array}\right]
= 
\begin{bmatrix}
U_{\eta,\eta} & U_{\eta,\kappa} & 0 &  0 & \cdots  &  0\\
U_{\kappa,\eta} & U_{\kappa,\kappa} & 0 &  0 & \cdots  &  0\\
U_{\phi,\eta} & U_{\phi,\kappa} & U_{\phi,\phi} & 0 & \cdots  &  0\\
U_{\partial_{x_1} \phi,\eta} & U_{\partial_{x_1} \phi,\kappa} & U_{\partial_{x_1} \phi,\phi} & U_{\partial_{x_1} \phi,\partial_{x_1} \phi} & \cdots  & 0 \\
&&\vdots & &\diagdown& \\
U_{\partial_{x_d} \phi,\eta} & U_{\partial_{x_d} \phi,\kappa} & U_{\partial_{x_d} \phi,\phi} & 0 & \cdots  & U_{\partial_{x_d} \phi,\partial_{x_d} \phi} 
\end{bmatrix}
  \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\partial_{x_1} \phi^t \\ \vdots \\ \delta\partial_{x_d} \phi^t \end{array}\right]
\end{equation}
where $U_{\kappa,\eta}= \bigl[ R(\kappa_i,\kappa_j) \bigr]_{i,j}$ for example. Now the transpose flow is given by \textcolor{red}{I think I need a minus sign in the equation below...}
\begin{equation}
\frac{d}{dt} \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\partial_{x_1} \phi^t  \\ \vdots \\ \delta\partial_{x_d} \phi^t   \end{array}\right]
= 
\begin{bmatrix}
U_{\eta,\eta}^T   & U_{\kappa,\eta}^T & U_{\phi,\eta}^T &  U_{\partial_{x_1} \phi,\eta}^T & \cdots  &  U_{\partial_{x_d} \phi,\eta}^T \\
U_{\eta,\kappa}^T & U_{\kappa,\kappa}^T & U_{\phi,\kappa}^T &  U_{\partial_{x_1} \phi,\kappa}^T & \cdots  &  U_{\partial_{x_d} \phi,\kappa}^T\\
0 & 0 & U_{\phi,\phi}^T & U_{\partial_{x_1} \phi,\phi}^T & \cdots  &  U_{\partial_{x_d} \phi,\phi}^T\\
0 & 0 & 0 & U_{\partial_{x_1} \phi,\partial_{x_1} \phi}^T & \cdots  & 0 \\
&&\vdots & &\diagdown& \\
0 & 0 &0 & 0 & \cdots  & U_{\partial_{x_d} \phi,\partial_{x_d} \phi} ^T
\end{bmatrix}
  \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\partial_{x_1} \phi^t \\ \vdots \\ \delta\partial_{x_d} \phi^t \end{array}\right]
\end{equation}
To unpack this matrix representation of the transpose flow lets start by interpreting the equation for $\delta\partial_{x_p} \phi^t$:
\begin{align}
\frac{d\delta\partial_{x_p}\phi_\ell}{dt} 
=&   \text{ $\ell^{\text{th}}$ segment of: $U_{\partial_{x_p} \phi,\partial_{x_p} \phi} ^T\delta\partial_{x_p}\phi$}
= \sum_{j=1}^N \underbrace{\bigl[ \nabla^T_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{d\times 1} \underbrace{\bigl[\eta_j^T\bigr]}_{1\times d}  \delta\partial_{x_p} {\phi}_\ell
\end{align}
notice that I didn't switch the $i$ and the $j$ here.
\begin{align}
\frac{d\delta\phi_\ell}{dt} &= \text{ $\ell^{\text{th}}$ segment of: } U_{\phi,\phi}^T\delta\phi  + U_{\partial_{x_1} \phi,\phi}^T\delta\partial_{x_1} \phi + \cdots  +  U_{\partial_{x_d} \phi,\phi}^T\delta\partial_{x_d} \phi \\
&=  \sum_{j=1}^N  \underbrace{\bigl[ \nabla^T_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{d\times 1}\underbrace{\bigl[\eta_j^T\bigr]}_{1\times d} \delta\phi_\ell \\
&\quad+\sum_{j=1}^N \underbrace{\bigl[\nabla_{\phi_\ell}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{d\times d} \underbrace{\bigl[\partial_{x_1} {\phi_\ell}\bigr]}_{d\times 1} \underbrace{\bigl[ \eta_j^T\bigr]}_{1\times d} \delta\partial_{x_1} \phi_\ell \\
 &\qquad\vdots \nonumber\\ 
 &\quad+ \sum_{j=1}^N \underbrace{\bigl[\nabla_{\phi_\ell}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_j)\bigr]}_{d\times d} \underbrace{\bigl[\partial_{x_d} {\phi_\ell}\bigr]}_{d\times 1} \underbrace{\bigl[ \eta_j^T\bigr]}_{1\times d} \delta \partial_{x_d}  \phi_\ell
\end{align}
For the transpose flow for $\kappa$ we get
\begin{align}
\frac{d\delta\kappa_i}{dt} &= \text{ $i^{\text{th}}$ segment of: }U_{\eta,\kappa}^T\delta\eta + U_{\kappa,\kappa}^T\delta\kappa + U_{\phi,\kappa}^T\delta\phi +  U_{\partial_{x_1} \phi,\kappa}^T\delta\partial_{x_1} \phi + \cdots  +  U_{\partial_{x_d} \phi,\kappa}^T\delta\partial_{x_d} \phi \\
&= \sum_{j=1}^N  \bigl[  \nabla^T_{\kappa_i}\nabla_{\kappa_i}R(\kappa_i,\kappa_j)  \bigr] \bigl[-\eta^T_i \eta_j\bigr] \delta\eta_i +\sum_{j=1}^N   \bigl[   \underbrace{ \{\nabla^T_{\kappa_i}\nabla_{\kappa_j}R(\kappa_j,\kappa_i)\} }_{d\times d}\bigr]{\bigl[-\eta^T_i \eta_j\bigr]}\delta\eta_j  \\
&\quad+\sum_{j=1}^N  \underbrace{\bigl[ \nabla^T_{\kappa_i} R(\kappa_i,\kappa_j)\bigr]}_{d\times 1} \underbrace{\bigl[\eta_j^T\bigr]}_{1\times d} \delta\kappa_i +\sum_{j=1}^N \bigl[ \nabla_{\kappa_i}^T R(\kappa_j,\kappa_i) \bigr]  \bigl[\eta_i^T\bigr] \delta\kappa_j \\
&\quad  + \sum_{\ell=1}^N \bigl[   \nabla^T_{\kappa_i}R(\phi_\ell,\kappa_i) \bigr] \bigl[\eta_i^T\bigr]\delta\phi_\ell \\
&\quad + \sum_{\ell=1}^N \underbrace{\bigl[\nabla_{\kappa_i}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_i)\bigr]}_{d\times d}     \underbrace{\bigl[\partial_{x_1} {\phi_\ell}\bigr]}_{d\times 1}  \underbrace{\bigl[\eta_i^T\bigr]}_{1 \times d}  \delta\partial_{x_1}\phi_\ell  \\
 &\qquad\vdots \nonumber\\ 
&\quad + \sum_{\ell=1}^N \underbrace{\bigl[\nabla_{\kappa_i}^T\nabla_{\phi_\ell}R(\phi_\ell,\kappa_i)\bigr]}_{d\times d}     \underbrace{\bigl[\partial_{x_d} {\phi_\ell}\bigr]}_{d\times 1}  \underbrace{\bigl[\eta_i^T\bigr]}_{1 \times d}  \delta\partial_{x_d}\phi_\ell .
\end{align}
Finally, for $\eta$ we get
\begin{align}
\frac{d\delta\eta_i}{dt} &= \text{ $i^{\text{th}}$ segment of: }  U_{\eta,\eta}^T\delta\eta + U_{\kappa,\eta}^T\delta\kappa + U_{\phi,\eta}^T\delta\phi +  U_{\partial_{x_1} \phi,\eta}^T\delta\partial_{x_1} \phi + \cdots  +  U_{\partial_{x_d} \phi,\eta}^T\delta\partial_{x_d} \phi \\
&=   \sum_{j=1}^N  \underbrace{\bigl[-  \eta_j   \bigr]}_{d\times 1} \underbrace{ \bigl[ \nabla_{\kappa_i}R(\kappa_i,\kappa_j)\bigr]}_{1\times d} \delta\eta_i  + \sum_{j=1}^N  \bigl[- \eta_j     \bigr] \bigl[\nabla_{\kappa_j}R(\kappa_j,\kappa_i)\bigr] \delta\eta_j   \\
&\quad+\sum_{j=1}^N  \bigl[R(\kappa_j,\kappa_i)\bigr] \delta\kappa_j  \\
 &\quad+ \sum_{\ell=1}^N \bigl[R(\phi_\ell,\kappa_i)\bigr] \delta\phi_\ell \\
 &\quad+\sum_{\ell=1}^N\underbrace{\bigl[\partial_{x_1}{\phi}_\ell^T \bigr]}_{1 \times d}  \underbrace{\bigl[ \nabla^T_{\phi_\ell}R(\phi_\ell,\kappa_i)\bigr]}_{d\times 1}  \delta\partial_{x_1}\phi_\ell\\
 &\qquad\vdots \nonumber\\ 
&\quad+\sum_{\ell=1}^N\underbrace{\bigl[\partial_{x_d}{\phi}_\ell^T \bigr]}_{1 \times d}  \underbrace{\bigl[ \nabla^T_{\phi_\ell}R(\phi_\ell,\kappa_i)\bigr]}_{d\times 1}  \delta\partial_{x_d}\phi_\ell
\end{align}


\paragraph{Deriving the transpose flow:}
It will be useful to write out how this flows forward in discrete time:
\begin{equation} 
\label{forwardTT}
 \left[\begin{array}{c} \delta\eta^1_i\\ \delta \kappa^1_i \\ \delta\phi^1(X_\ell) \\ \delta \nabla\phi^1(X_\ell)   \end{array}\right]
 = \Bigl[I+\epsilon U_{t_N} \Bigr]\times \cdots \times  \Bigl[I+\epsilon U_{t_0} \Bigr]  \left[\begin{array}{c} \delta\eta^0_i\\ \delta \kappa^0_i \\ \delta\phi^0(X_\ell) \\ \delta \nabla\phi^0(X_\ell)   \end{array}\right].
\end{equation}
To derive the transpose flow note the effect of the rate of change on the energy notice that when perturbing $\eta^0_i$ and $\kappa^0_i$ one gets a perturbation  $\phi^t(X_\ell) +\epsilon \delta \phi^t(X_\ell)$. Notice that $\delta \phi$ is a Eulerian specification of a flow field.  To switch to Lagrangian specification on defines $u^t(x)$ as satisfying the identity
\[ u^t (\phi^t(x))=\delta \phi^t(x)\]
Earlier we derive the rate of change of the log likelihood at time $t=1$ with respect to Lagrangian coordinates as follows
\[  \frac{1}{n} \sum_{\ell=1}^n \text{div}\, u (\phi^1(X_\ell)) + \bigl\langle \nabla H (\phi^1(X_\ell)), u^1(\phi^1(X_\ell))\bigr\rangle.  \]
 To switch to the Eulerian specification notice that $D\delta \phi^1 (x) =  Du^1 (\phi^1(x)) D\phi^1(x) $. Therefore $\text{trace} ([D\delta \phi^1 (x)][ D\phi^1(x)]^{-1} ) = \text{trace}\bigl( Du^1 (\phi^1(x))\bigr)  =  \text{div}\, u (\phi^1(x))$ and hence the rate of change of the log likelihood can be computed as
\begin{equation} 
\label{Eulerian}
\frac{1}{n} \sum_{\ell=1}^n \text{trace} ([ D\phi^1(X_\ell)]^{-1} [D\delta \phi^1 (X_\ell)]) + \bigl\langle \nabla H (\phi^1(X_\ell)), \delta \phi^1(X_\ell)\bigr\rangle  
\end{equation}

\textcolor{red}{Don't forget about the regularization term}

Not to see how to get the transpose flow notice that equation (\ref{Eulerian}) shows that
\begin{align*}
\frac{d}{d\epsilon}\Bigr|_{\epsilon = 0}& \text{loglike}(\eta_i^0+\epsilon \delta \eta_i^0, \kappa^0_i+\epsilon \delta \kappa_i^0) \\
&=\underbrace{ \left[ 0,0,\frac{\nabla H (\phi^1(X_\ell))}{n}, \frac{[\nabla\phi^1(X_\ell)]^{-1}}{n}  \right]}_{\text{ the gradient w.r.t the parameters at time $t=1$}} \left[\begin{array}{c} \delta\eta^1_i\\ \delta \kappa^1_i \\ \delta\phi^1(X_\ell) \\ \delta \nabla\phi^1(X_\ell)   \end{array}\right] \\
&\underset{=}{ \text{by (\ref{forwardTT})}} \underbrace{ \left[ 0,0,\frac{\nabla H (\phi^1(X_\ell))}{n}, \frac{[\nabla\phi^1(X_\ell)]^{-1}}{n}  \right] \Bigl[I+\epsilon U_{t_N} \Bigr]\times \cdots \times  \Bigl[I+\epsilon U_{t_0} \Bigr] }_{\text{ the gradient w.r.t the parameters at time $t=0$}}  \left[\begin{array}{c} \delta\eta^0_i\\ \delta \kappa^0_i \\ \delta\phi^0(X_\ell) \\ \delta \nabla\phi^0(X_\ell)   \end{array}\right]
\end{align*}
Now to compute the gradient w.r.t the parameters at time $t=0$, call it $\nabla \ell^0$, one can take a transpose:
\begin{equation}
\label{oouoo}
\nabla \ell^0 =\Bigl[I+\epsilon U_{t_0}^T \Bigr]\times \cdots \times \Bigl[I+\epsilon U_{t_N}^T \Bigr] \left[ 0,0,\frac{\nabla H (\phi^1(X_\ell))}{n}, \frac{[\nabla\phi^1(X_\ell)]^{-1}}{n}  \right]^T.  
\end{equation}
Notice that the above formula gives us a continuos ODE flow for the likelihood as follows:
\begin{equation}
\label{likeforward} 
\frac{d}{dt} \nabla\ell^t = -U_t^T   \nabla\ell^t
\end{equation}
with initial condition set at time $t=1$ as follows $ \nabla\ell^1 = \left[ 0,0,\frac{\nabla H (\phi^1(X_\ell))}{n}, \frac{[\nabla\phi^1(X_\ell)]^{-1}}{n}  \right]^T$. Indeed, to flow (\ref{likeforward}) forward in time one would use the formula: $\nabla \ell^{t+\epsilon} = \nabla \ell^{t} + \epsilon  [-U_t^T]   \nabla\ell^t$. To flow  (\ref{likeforward}) backward in time one would use the formula: $\nabla \ell^{t-\epsilon} = \nabla \ell^{t} - \epsilon  [-U_t^T]   \nabla\ell^t = (I + \epsilon  U_t^T)  \nabla\ell^t$ which has discrete analog:
\[ \nabla \ell^{t_0} =  [I + \epsilon  U_{t_{0}}^T]\times\cdots\times [I + \epsilon  U_{t_N}^T] \nabla\ell^{t_N}
\]
which agrees with (\ref{oouoo}).
\begin{algorithm}[h!]
\caption{Compute the gradient of the log likelihood at initial $\eta_\text{\tiny init},\kappa_\text{\tiny init}$} 
\label{alg2} 
\begin{algorithmic}[1]
\STATE {Solve $(\eta^t,\kappa^t, \phi^t, \nabla \phi^t)^T$ {\it forward in time},  starting at $t=0$ and finishing at $t=1$, with the ODE given in equations. Use initial conditions $(\eta^0,\kappa^0,\phi^0_i, \nabla \phi_i^0)^T= (\eta_\text{\tiny init},\kappa_\text{\tiny init}, X_i, I)^T$.}
\STATE{Solve $(\delta\eta^t,\delta\kappa^t, \delta\phi^t, \delta\nabla \phi^t)^T$ {\it backward in time}, starting at $t=1$ and finishing at $t=0$, by running the ODE
\begin{equation}
\frac{d}{dt} \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta \nabla\phi^t   \end{array}\right]
= -U_t^T  \left[\begin{array}{c} \delta\eta^t\\ \delta \kappa^t \\ \delta\phi^t \\ \delta\nabla\phi^t   \end{array}\right]
\end{equation}
with initial conditions 
$(\delta \eta^1,\delta \kappa^1, \delta\phi^1, \delta{\nabla \phi}^1)^T:= \bigl(0,0,{\nabla H (\phi^1(X_\ell))}/{n}, {[\nabla\phi^1(X_\ell)]^{-1}}/{n}\bigr ) $. Note: you will need to simultaniously  solve $(\eta^t,\kappa^t, \phi^t, \nabla \phi^t)^T$ {\it backward in time} down from $t=1$  to compute $U_t$.}
\STATE{Return: $\delta\eta_\text{\tiny init} = \delta \eta^0$ and $\delta\kappa_\text{\tiny init} = \delta \kappa^0$.}
\end{algorithmic}
\end{algorithm}


%
{\flushleft\textcolor{blue}{$\uparrow$------------end long version---------}}\newline
} \fi
%%------------------------------ end long version


\bibliography{refs}

\end{document}
